[
  {
    "objectID": "02-models.html",
    "href": "02-models.html",
    "title": "2  Models",
    "section": "",
    "text": "2.1 Tidymodels overhead\nWhile the tidymodels team develops the infrastructure that users interact with directly, under the hood, we send calls out to other people’s modeling packages—or modeling engines—that provide the actual implementations that estimate parameters, generate predictions, etc. The process looks something like this:\nWhen thinking about the time allotted to each of the three steps above, we refer to the “translate” steps in green as the tidymodels overhead. The time it takes to “translate” interfaces in steps 1) and 3) is within our control, while the time the modeling engine takes to do it’s thing in step 2) is not.\nLet’s demonstrate with an example classification problem. Generating some random data:\nd &lt;- simulate_classification(n_rows = 100)\n\nd\n\n# A tibble: 100 × 18\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;   &lt;fct&gt;               &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_1 level_1            1.75   level_4           0.361          0.231\n 2 class_2 level_1           -0.0230 level_2           0.0840         0.609\n 3 class_2 level_4           -2.32   level_4           0.273          0.495\n 4 class_2 level_1            0.315  level_2           0.559          0.952\n 5 class_2 level_4            0.701  level_2           0.249          0.321\n 6 class_2 level_1            0.593  level_2           0.701          0.935\n 7 class_1 level_1            0.871  level_2           0.373          0.986\n 8 class_1 level_4            0.656  level_3           0.543          0.258\n 9 class_1 level_1            1.94   level_2           0.506          0.424\n10 class_2 level_1           -0.585  level_5           0.00916        0.207\n# ℹ 90 more rows\n# ℹ 12 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;fct&gt;, linear_11 &lt;fct&gt;,\n#   linear_12 &lt;fct&gt;\n…we’d like to model the class using the remainder of the variables in this dataset using a logistic regression. We can using the following code to do so:\nfit(logistic_reg(), class ~ ., d)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = class ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n        (Intercept)  two_factor_1level_2  two_factor_1level_3  \n            2.07900             -2.25506             -0.65084  \ntwo_factor_1level_4         two_factor_2  non_linear_1level_2  \n           -3.41669             -1.12578              0.85830  \nnon_linear_1level_3  non_linear_1level_4  non_linear_1level_5  \n           -1.29977              0.91646              1.45017  \n       non_linear_2         non_linear_3            linear_01  \n            0.82405             -0.51534              0.48283  \n          linear_02            linear_03            linear_04  \n           -0.47137              0.27891             -0.59696  \n          linear_05            linear_06            linear_07  \n           -0.02195              0.37964             -0.41534  \n          linear_08            linear_09  \n           -0.05485             -0.23599  \n [ reached getOption(\"max.print\") -- omitted 7 entries ]\n\nDegrees of Freedom: 99 Total (i.e. Null);  73 Residual\nNull Deviance:      125.4 \nResidual Deviance: 79.45    AIC: 133.5\nThe default engine for a logistic regression in tidymodels is stats::glm(). So, in the style of the above graphic, this code:\nAgain, we can control what happens in steps 1) and 3), but step 2) belongs to the stats package.\nThe time that steps 1) and 3) take is relatively independent of the dimensionality of the training data. That is, regardless of whether we train on one hundred or a million data points, our code (as in, the translation) takes about the same time to run. Regardless of training set size, our code pushes around small, relational data structures to determine how to correctly interface with a given engine. The time it takes to run step 2), though, depends almost completely on the size of the data. Depending on the modeling engine, modeling 10 times as much data could result in step 2) taking twice as long, or 10x as long, or 100x as long as the original fit.\nSo, while the absolute time allotted to steps 1) and 3) is fixed, the portion of total time to fit a model with tidymodels that is “overhead” depends on how quick the engine code itself is. How quick is a logistic regression with glm() on 100 data points?\nbench::mark(\n  fit = glm(class ~ ., family = binomial, data = d)\n) %&gt;% \n  select(expression, median)\n\n# A tibble: 1 × 2\n  expression   median\n* &lt;bch:expr&gt; &lt;bch:tm&gt;\n1 fit          2.58ms\nAbout a millisecond. That means that, if the tidymodels overhead is one second, we’ve made this model fit a thousand times slower!\nIn practice, the overhead here has hovered around a millisecond or two for the last couple years, and machine learning practitioners usually fit much more computationally expensive models than a logistic regression on 100 data points. You’ll just have to believe me on that second point. Regarding the first:\nbm_logistic_reg &lt;- \n  bench::mark(\n    parsnip = fit(logistic_reg(), class ~ ., d),\n    stats = glm(class ~ ., family = binomial, data = d),\n    check = FALSE\n  )\nRemember that the first expression calls the second one, so the increase in time from the second to the first is the “overhead.” In this case, it’s 0.7715381 milliseconds, or 22.7% of the total elapsed time.\nSo, to fit a boosted tree model on 1,000,000 data points, step 2) might take a few seconds. Steps 1) and 3) don’t care about the size of the data, so they still take a few thousandths of a second. No biggie—the overhead is negligible. Let’s quickly back that up by fitting boosted tree models on simulated datasets of varying sizes, once with the XGBoost interface and once with parsnip’s wrapper around it.\nThis graph shows the gist of tidymodels’ overhead for modeling engines: as dataset size and model complexity grow larger, model fitting and prediction take up increasingly large proportions of the total evaluation time.\nSection 1.1.3 showed a number of ways users can cut down on the evaluation time of their tidymodels code. Making use of parallelism, reducing the total number of model fits needed to search a given grid, and carefully constructing that grid to search over are all major parts of the story",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "02-models.html#tidymodels-overhead",
    "href": "02-models.html#tidymodels-overhead",
    "title": "2  Models",
    "section": "",
    "text": "A graphic representing the tidymodels interface. In order, step 1 “translate”, step 2 “call”, and step 3 “translate”, outline the process of translating from the standardized tidymodels interface to an engine’s specific interface, calling the modeling engine, and translating back to the standardized tidymodels interface. Step 1 and step 3 are in green, while step 2 is in orange.\n\n\n\n\n\n\n\n\n\nTranslates the tidymodels code, which is consistent across engines, to the format that is specific to the chosen engine. In this case, there’s not a whole lot to do: it passes the preprocessor as formula, the data as data, and picks a family of stats::binomial.\nCalls stats::glm() and collects its output.\nTranslates the output of stats::glm() back into a standardized model fit object.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "02-models.html#benchmarks",
    "href": "02-models.html#benchmarks",
    "title": "2  Models",
    "section": "2.2 Benchmarks",
    "text": "2.2 Benchmarks\n\n2.2.1 Linear models\n\n\n2.2.2 Decision trees\n\n\n2.2.3 Boosted trees\nXGBoost and LightGBM – comparison timings for the same thing but from the Python interface?\n\n\n2.2.4 Random forests\n\n\n2.2.5 Support vector machines",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html",
    "href": "03-parallelism.html",
    "title": "3  Parallel computing",
    "section": "",
    "text": "3.1 Across-models\nIn Chapter 1, one of the changes I made to greatly speed up that resampling process was to introduce an across-models parallel backend. By “across-models,” I mean that each individual model fit happens on a single CPU core, but I allot each of the CPU cores I’ve reserved for training a number of model fits to take care of.\nPhrased another way, in the case of “sequential” training, all 120 model fits happened one after the other.\nWhile the one CPU core running my R process works itself to the bone, the other remaining 9 are mostly sitting idle (besides keeping my many browser tabs whirring). CPU parallelism is about somehow making use of more cores than the one my main R process is running on; in theory, if \\(n\\) times as many cores are working on fitting models, the whole process could take \\(1/n\\) of the time.\nWhy do I say “in theory”? The orchestration of splitting up that work is actually a very, very difficult problem, for two main reasons:\nThere are two dominant approaches to distributing model fits across local cores that I’ve hinted at already: forking and socket clusters. We’ll delve further into the weeds of each of those approaches in the coming subsections. At a high level, though, the folk knowledge is that forking is subject to less overhead in sending data back and forth, but has some quirks that make it less portable (more plainly, it’s not available on Windows) and a bit unstable thanks to a less-than-friendly relationship with R’s garbage collector. As for load balancing, the choice between these two parallelism techniques isn’t really relevant.\nBefore I spend time experimenting with these techniques, I want to quickly situate the terminology I’m using here in the greater context of discussions of parallel computing with R. “Sequential,” “forking,” and “socket clusters” are my preferred terms for the techniques I’ll now write about, but there’s quite a bit of diversity in the terminology folks use to refer to them. I’ve also called out keywords (as in, functions or packages) related to these techniques in various generations of parallel computing frameworks in R. In “base,” I refer to functions in the parallel package, building on popular packages multicore (first on CRAN in 2009, inspiring mclapply()) and snow (first on CRAN in 2003, inspiring parLapply()) and included in base installations of R from 2011 onward (R Core Team 2024).",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-across-models",
    "href": "03-parallelism.html#sec-across-models",
    "title": "3  Parallel computing",
    "section": "",
    "text": "Figure 3.1\n\n\n\n\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\nGetting data from one place to another: Each of us has likely spent minutes or even hours waiting for a call to load() on a big old .RData object to complete. In that situation, data is being brought in from our hard disk or a remote database (or even elsewhere in memory) into memory allocated by our R process. R is a single-process program, so in order to train multiple models simultaneously, we need multiple R processes, each able to access the training data. We then have two options: one would be to somehow allow each of those R processes to share one copy of the data (this is the idea behind forking, described in Section 3.1.2), the other to send a copy of the data to each process (the idea behind socket clusters, described in Section 3.1.3). The former sounds nice but can become a headache quite quickly. The latter sounds computationally expensive but, with enough memory and sufficiently low latency in copying data (as would be the case with a set of clusters living on one laptop), can often outperform forking for local workflows.\nLoad balancing: Imagine I have some machine learning model with a hyperparameter \\(p\\), and that the computational complexity of that model is such that, with hyperparameter value \\(p\\), the model takes \\(p\\) minutes to train. I am trying out values of \\(p\\) in \\(1, 2, 3, ..., 40\\) and distributing model training across 5 cores. Without the knowledge of how \\(p\\) affects training times, I might send models with \\(p\\) in \\(1, 2, 3, ...8\\) off to the first core, \\(9, 10, 11, ..., 16\\) off to the second core, and so on. In this case, the first core would finish up all of its fits in a little over half an hour while the last would take almost 5 hours. In this example, I’ve taken the penalty on overhead of sending all of the training data off to each core, but in the end, one core ends up doing the majority of the work anyway. In this case, too, we were lucky that the computational complexity of model fits relative to this parameter were roughly linear—it’s not uncommon for model fit times to have a quadratic or geometric relationship with the values of important hyperparameters. A critical reader might have two questions. The first: if the computational complexity relative to this parameter is known, why don’t you just batch the values of \\(p\\) up such that each worker will take approximately the same amount of time? This is a reasonable question, and it relates to the hard problem of chunking. In some situations, related to individual parameters, it really is just about this simple to determine the relationship between parameter values and fit times. In reality, those relationships tend not to be quite so clear-cut, and even when they are, the implications of that parameter value for fit times often depend on the values of other parameters; a pairing of some parameter value \\(p\\) with some other value of a different parameter \\(q\\) might cause instability in some gradient descent process or otherwise, making the problem of estimating the fit time of a model given some set of parameter values a pretty difficult problem for some model types. The second question: couldn’t you just send each of the cores a single parameter value and have them let the parent R process know they’re done, at which point they’ll receive another parameter value to get to work on evaluating? That way, the workers that happen to end up with a quicker-fitting values earlier on won’t sit idle waiting for other cores to finish. This approach is called asynchronous (or “async”) and, in some situations, can be quite helpful. Remember, though, that this requires getting data (in the form of the communication that a given worker is done evaluating a model, and maybe passing along some performance metric values) back and forth much more often. If the overhead of that communication exceeds the time that synchronous workers had spent idle, waiting for busier cores to finish running, then the asynchronous approach will result in a net slowdown.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nfuture\nforeach\nbase\nSynonyms\n\n\n\n\nSequential\nsequential()\n\n\nSerial\n\n\nForking\nmulticore()\ndoMC\nmclapply()\nProcess forking\n\n\nSocket Clusters\nmultisession()\ndoParallel\nparLapply()\nParallel Socket Clusters (PSOCK), Cluster, Socket\n\n\n\n\n3.1.1 Sequential\nGenerally, in this chapter, I’m writing about various approaches to parallel computing. I’ll compare each of those approaches to each, but also to the sequential (or “not parallel”) approach. Sequential evaluation means evaluating model fits in sequence, or one after the other.\nTo demonstrate the impacts of different parallelism approaches throughout this chapter, we’ll always start the conversation with a short experiment. I’ll define a function that tracks the elapsed time to resample a boosted tree ensemble against simulated data, given a number of rows to simulate and a parallelism approach.\n\ntime_resample_bt &lt;- function(n_rows, plan) {\n  # simulate data with n_rows rows\n  set.seed(1)\n  d &lt;- simulate_regression(n_rows)\n  \n  # set up a parallelism plan\n  # set `workers = 4`, which will be ignored for `plan = \"sequential\"`.\n  if (plan == \"multicore\") {\n    rlang::local_options(parallelly.fork.enable = TRUE)\n  }\n  if (plan == \"multisession\") {\n    rlang::local_options(future.globals.maxSize = 1024*1024^2) # 1gb\n  }\n  suppressWarnings(\n    plan(plan, workers = 4) \n  )\n  \n  # track the elapsed time to...\n  bench::mark(\n    resample =\n      fit_resamples(\n        # ...evaluate a boosted tree ensemble...\n        boost_tree(\"regression\"),\n        # ...modeling the outcome using all predictors...\n        outcome ~ .,\n        # ...against a 10-fold cross-validation of `d`.\n        vfold_cv(d, v = 10)\n      ),\n    memory = FALSE\n  )\n}\n\nHere’s a quick example, simulating 100 rows of data and evaluating its resamples sequentially:\n\nt_seq &lt;- time_resample_bt(100, \"sequential\")\n\nt_seq\n\n\nt_seq\n\n# A tibble: 1 × 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      201ms    2.19MB\n\n\nIn total, the whole process took 0.2 seconds on my laptop. This expression, among other things, fits a model for each resample on \\(n * \\frac{v-1}{v} = 100 * \\frac{9}{10} = 90\\) rows, meaning that even if the model fits took up 100% of the evaluation time in total, they take 0.02 seconds each. In other words, these fits are quite fast. As such, the overhead of distributing computations across cores would have to be quite minimal in order to see speedups with computations done in parallel. Scaling up the number of rows in the training data, though, results in elapsed times becoming a bit more cumbersome; in the following code, we’ll resample models on datasets with 100 to a million rows, keeping track of the elapsed time for each iteration.\n\npress_seq &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"sequential\"),\n    n_rows = 10^(2:6)\n  )\n\nFor now, the graph we can put together with this data isn’t super interesting:\n\nggplot(press_seq) +\n  aes(x = n_rows, y = median) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\n\n\n\n\nMore rows means a longer fit time—what a thrill! In the following sections, we’ll compare this timing to those resulting from different parallelism approaches.\n\n\n3.1.2 Forking\nProcess forking is a mechanism where an R process creates an exact copy of itself, called a “child” process (or “worker.”) Initially, workers share memory with the original (“parent”) process, meaning that there’s no overhead resulting from creating multiple copies of training data to send out to workers. So, in the case of tidymodels, each worker needs to have some modeling packages loaded and some training data available to get started on evaluating a model workflow against resample; those packages and data are already available in the parent process, so tidymodels should see very little overhead in shipping data off to workers with forking.\n\nThere are a few notable drawbacks of process forking:\n\nThere’s no direct way to “fork” a process from one machine to another, so forking is available only on a single machine. To distribute computations across multiple machines, practitioners will need to make use of socket clusters (described in the following section Section 3.1.3).\nForking is based on the operating system command fork, available only on Unix-alikes (i.e. macOS and Linux). Windows users are out of luck.\nIn practice, memory that is initially shared often ends up ultimately copied due to R’s garbage collection. “[I]f the garbage collector starts running in one of the forked [workers], or the [parent] process, then that originally shared memory can no longer be shared and the operating system starts copying memory blocks into each [worker]. Since the garbage collector runs whenever it wants to, there is no simple way to avoid this” (Bengtsson 2021).\n\nLet’s rerun that experiment from the previous section using forking and compare timings.\n\npress_fork &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"multicore\"),\n    n_rows = 10^(2:6)\n  )\n\nWe can make the plot from the last section a bit more interesting now.\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\")\n) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\n\n\n\n\nWell, if that ain’t by the book! For the smallest training dataset, n = 100, distributing computations across cores resulted in a new slowdown. Very quickly, though, forking meets up with the sequential approach in elapsed time, and by the time we’ve made it to more realistic dataset sizes, forking almost always wins. In case the log scale is tripping you up, here are the raw timings for the largest dataset:\n\n\n# A tibble: 2 × 3\n    median Approach    n_rows\n* &lt;bch:tm&gt; &lt;chr&gt;        &lt;dbl&gt;\n1    4.13m Sequential 1000000\n2    1.34m Forking    1000000\n\n\n\nAnother angle from which to poke at this is how much time did we lose to overhead? Said another way, if I distributed these computations across 4 cores, then I should see a 4-fold speedup in a perfect world. If I divide the time it takes to resample this model sequentially by 4, how does it compare to the timing I observe with forking?\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\")\n) %&gt;%\n  mutate(median_adj = case_when(\n    Approach == \"Sequential\" ~ median / 4,\n    .default = median\n  )) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median_adj, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time Per Core)\", x = \"Number of Rows\")\n\n\n\n\n\n\n\n\nPer core, no parallel approach will ever be faster than sequential. (If it is, there’s a performance bug in your code!) As the computations that happen in workers take longer and longer, though, the overhead shrinks as a proportion of the total elapsed time.\n\n\n3.1.3 Socket Clusters\n\nLet’s see how this plays out in practice. Returning to the same experiment from before:\n\npress_sc &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"multisession\"),\n    n_rows = 10^(2:6)\n  )\n\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\"),\n  mutate(press_sc, Approach = \"Socket Clusters\")\n) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-within-models",
    "href": "03-parallelism.html#sec-within-models",
    "title": "3  Parallel computing",
    "section": "3.2 Within-models",
    "text": "3.2 Within-models\nI’ve only written so far about across-model parallelism, where multiple CPU cores are used to train a set of models but individual model fits happen on a single core. For most machine learning models available via tidymodels, this is the only type of parallelism possible. However, some of the most well-used modeling engines in tidymodels, like XGBoost and LightGBM, allow for distributing the computations to train a single model across several CPU cores (or even on GPUs). This begs the question, then, of whether tidymodels users should always stick to across-model parallelism, to within-model parallelism for the models it’s available for, or to a hybrid of both. We’ll focus first on the former two options in this subsection and then explore their interactions in Section 3.3.\n\n3.2.1 CPU\n\n\n3.2.2 GPU\nXGBoost parallelizes at a finer level (individual trees and split finding), while LightGBM parallelizes at a coarser level (features and data subsets).\nXGBoost\n\narg device is cpu or cuda (or gpu, but cuda is the only supported device).\narg nthread is integer\nuses openMP for cpu (?)\ngpu_hist is apparently pretty ripping?\n\nLightGBM\n\ndevice_type is cpu, gpu, or cuda (fastest, but requires GPUs supporting CUDA)\ncuda is fastest but only available on Linux with NVIDIA GPUs with compute capability 6.0+\ngpu is based on OpenCL… M1 Pro is a “built-in” / “integrated”\ncpu uses OpenMP for CPU parallelism\nset to real CPU cores (i.e. not threads)\nrefer to Installation Guide to build LightGBM with GPU or CUDA support\nare num_threads is integer\nDask available to Python users\n\naorsf\n\nn_thread implements OpenMP CPU threading\n\nkeras (MLP)\n\nCPU or GPU\ncuda (available only for jax backend?)\nalso available with tensorflow\n\nh2o stuff\nbaguette\n\ncontrol_bag(allow_parallel)? does this respect nested parallelism?\n\nWhile GPU support is available for both libraries in R, it’s not as straightforward to use as in Python. The R ecosystem generally has less robust GPU support compared to Python, which can make GPU-accelerated machine learning in R more challenging to set up and use.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-within-and-across",
    "href": "03-parallelism.html#sec-within-and-across",
    "title": "3  Parallel computing",
    "section": "3.3 Within and Across",
    "text": "3.3 Within and Across",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#strategy",
    "href": "03-parallelism.html#strategy",
    "title": "3  Parallel computing",
    "section": "3.4 Strategy",
    "text": "3.4 Strategy\nChoosing n cores and parallel_over…",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#distributed-computing",
    "href": "03-parallelism.html#distributed-computing",
    "title": "3  Parallel computing",
    "section": "3.5 Distributed Computing",
    "text": "3.5 Distributed Computing\nSo far in this chapter, I’ve mostly focused on the performance considerations for distributing computations across cores on a single computer. Distributed computing “in the cloud,” where data is shipped off for processing on several different computers, is a different ball-game. That conversation is mostly outside of the scope of this book, but I do want to give some high-level intuition on how local parallelism differs from distributed parallelism.\nThe idea that will get you the most mileage in reasoning about distributed parallelism is this: the overhead of sending data back and forth is much more substantial in distributed computing than it is in the local context.\nIn order for me to benefit from distributing these computations across cores, the overhead of sending data out to workers has to be so minimal that it doesn’t overtake the time saved in distributing model fits across cores. Let’s see how this plays out on my laptop, first:\n\nt_par &lt;- time_resample_bt(100, \"multisession\")\n\nt_par\n\n\nt_par\n\n# A tibble: 1 × 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      242ms      15MB\n\n\nNo dice! As we know from earlier on, though, we’ll start to see a payoff when model fits take long enough to outweigh the overhead of sending data back and forth between workers. But, but! Remember the high-mileage lesson: this overhead is greater for distributed systems. Let’s demonstrate this.\nI’ll first run this experiment for numbers of rows \\(100, 1000, ..., 1,000,000\\) both sequentially and via socket clusters on my laptop, recording the timings as I do so. Then, I’ll do the same thing on a popular data science hosted service, and we’ll compare results.\n\nbench::press(\n  time_resample_bt(n_rows, plan),\n  n_rows = 10^(2:6),\n  plan = c(\"sequential\", \"multisession\")\n)\n\nThe resulting timings are in the object timings and look like this:\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n   n_rows plan           median system\n*   &lt;dbl&gt; &lt;chr&gt;        &lt;bch:tm&gt; &lt;chr&gt; \n1     100 sequential   216.08ms laptop\n2    1000 sequential   442.77ms laptop\n3   10000 sequential      2.81s laptop\n4  100000 sequential      27.8s laptop\n5 1000000 sequential      4.03m laptop\n6     100 multisession 247.13ms laptop\n\n\nThere’s one timing per unique combination of number of rows, parallelism plan, and system (\"laptop\" vs \"cloud\"). Based on these timings, we can calculate the factor of speedup for sequential evaluation versus its parallel analogue.\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  pivot_wider(names_from = plan, values_from = median) %&gt;%\n  mutate(\n    speedup = as.numeric(sequential / multisession),\n    speedup = if_else(speedup &lt; 1, -1/speedup, speedup)\n  ) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = speedup, col = system) +\n  geom_point() +\n  scale_x_log10() +\n  labs(\n    x = \"Log(Number of Rows)\", \n    y = \"Signed Factor of Speedup\", \n    col = \"System\"\n  )\n\n\n\n\n\n\n\n\nIn this plot, a signed speedup value of 2 would mean that the socket cluster (i.e., parallel) approach was twice as fast, while a value of -2 would mean that the sequential approach ran twice as fast as the socket cluster approach. In general, larger numbers of rows (and thus longer-running model fits) tend to be associated with greater speedups as a result of switching to parallel computing. For local clusters on my laptop, the overhead of passing data around is small enough that I start to see a payoff when switching to parallel computing for only 1000 rows. As for the cloud system, though, model fits have to take a long time before switching to parallel computing begins to reduce elapsed times.\nThe conclusion to draw here is not that one ought not to use hosted setups for parallel computing. Aside from the other many benefits of doing data analysis in hosted environments, some of these environments enable “massively parallel” computing, or, in other words, a ton of cores. In the example shown in this chapter, we fitted a relatively small number of models, so we wouldn’t necessarily benefit (and would likely see a slowdown) from scaling up the number of cores utilized. If we had instead wanted to tune that boosted tree over 1,000 proposed hyperparameter combinations—requiring 10,000 model fits with a 10-fold resampling scheme—we would likely be much better off utilizing a distributed system with higher latency (and maybe even less performant individual cores) and 1,000 or 10,000 cores.\n\n\n\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and Distributed Processing in R using Futures.” The R Journal 13 (2): 273–91. https://doi.org/10.32614/RJ-2021-048.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "04-search.html",
    "href": "04-search.html",
    "title": "4  Search",
    "section": "",
    "text": "4.1 Grid search",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "04-search.html#iterative-search",
    "href": "04-search.html#iterative-search",
    "title": "4  Search",
    "section": "4.2 Iterative Search",
    "text": "4.2 Iterative Search\n\n4.2.1 Simulated Annealing\n\n\n4.2.2 Bayesian Optimization",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "05-submodel.html",
    "href": "05-submodel.html",
    "title": "5  The submodel trick",
    "section": "",
    "text": "5.1 Demonstration\nRecall that, in Section 1.1.2, we resampled an XGBoost boosted tree model to predict whether patients would be readmitted within 30 days after an inpatient hospital stay. Using default settings with no optimizations, the model took 3.68 hours to resample. With a switch in computation engine, implementation of parallel processing, change in search strategy, and enablement of the submodel trick, the time to resample was knocked down to 1.52 minutes. What is the submodel trick, though, and what was its individual contribution to that speedup?\nFirst, loading packages and setting up resamples and the model specification as before:\n# load packages\nlibrary(tidymodels)\nlibrary(readmission)\n\n# load and split data:\nset.seed(1)\nreadmission_split &lt;- initial_split(readmission)\nreadmission_train &lt;- training(readmission_split)\nreadmission_test &lt;- testing(readmission_split)\nreadmission_folds &lt;- vfold_cv(readmission_train)\n\n# set up model specification\nbt &lt;- \n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "05-submodel.html#demonstration",
    "href": "05-submodel.html#demonstration",
    "title": "5  The submodel trick",
    "section": "",
    "text": "5.1.1 Grids\nIf I just pass these resamples and model specification to tune_grid(), as I did in Section 1.1.2, tidymodels will take care of generating the grid of parameters to evaluate itself. By default, tidymodels generates grids of parameters using an experimental design called a latin hypercube (McKay, Beckman, and Conover 2000; Dupuy, Helbert, and Franco 2015). We can use the function grid_latin_hypercube() from the dials package to replicate the same grid that tune_grid() had generated under the hood:\n\nset.seed(1)\nbt_grid_latin_hypercube &lt;- \n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_latin_hypercube(size = 12)\n\nbt_grid_latin_hypercube\n\n# A tibble: 12 × 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1   647    0.164  \n 2   404    0.0555 \n 3  1798    0.0452 \n 4   796    0.274  \n 5  1884    0.00535\n 6  1139    0.00137\n 7  1462    0.105  \n 8  1201    0.00194\n 9   867    0.0225 \n10   166    0.00408\n11   326    0.0116 \n12  1608    0.00997\n\n\nSince we’re working with a two-dimensional grid in this case, we can plot the resulting grid to get a sense for the distribution of values:\n\nggplot(bt_grid_latin_hypercube) +\n  aes(x = trees, y = learn_rate) +\n  geom_point()\n\n\n\n\n\n\n\n\nWhile the details of sampling using latin hypercubes are not important to understand this chapter, note that values are not repeated in a given dimension. Said another way, we get 12 unique values for trees, and 12 unique values for learn_rate. Juxtapose this with the design resulting from grid_regular() used in Section 1.1.3:\n\nset.seed(1)\nbt_grid_regular &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\nbt_grid_regular\n\n# A tibble: 16 × 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1     1    0.001  \n 2   667    0.001  \n 3  1333    0.001  \n 4  2000    0.001  \n 5     1    0.00681\n 6   667    0.00681\n 7  1333    0.00681\n 8  2000    0.00681\n 9     1    0.0464 \n10   667    0.0464 \n11  1333    0.0464 \n12  2000    0.0464 \n13     1    0.316  \n14   667    0.316  \n15  1333    0.316  \n16  2000    0.316  \n\n\nPlotting in the same way:\n\nggplot(bt_grid_regular) +\n  aes(x = trees, y = learn_rate) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe argument levels = 4 indicates that \\(4\\) values are generated individually for each parameter, and the resulting grid is created by pairing up each unique combination of those values.\nYou may have noticed that this regular grid contains even more proposed points—4 x 4 = 16—than the latin hypercube with size = 12. A reasonable question, then: how on earth would the larger grid be resampled more quickly than the smaller one? It’s possible that I hid some slowdown resulting from this larger grid among the rest of the optimizations implemented in Section 1.1.3; lets test the effect of the change in grid by itself.\n\nset.seed(1)\n\nbm_grid_regular &lt;- \n  bench::mark(\n    grid_regular = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid_regular\n      )\n  )\n\n\nbm_grid_regular\n\n# A tibble: 1 × 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 basic         1.23h    3.91GB\n\n\nSee? Have some faith in me!\nChanging only the grid argument (and even increasing the number of proposed grid points), we’ve decreased the time to evaluate against resamples from 3.68 to 1.23 hours, or a speedup of -99.67%.\n\n\n5.1.2 The trick\nPassing this regular grid allowed tune_grid() to use what the tidymodels team refers to as the “submodel trick,” where many more models can be evaluated than were actually fit.\nTo best understand how the submodel trick works, let’s refresh on how boosted trees work. The training process begins with a simple decision tree, and subsequent trees are added iteratively, each one correcting the errors of the previous trees by focusing more on the data points associated with the greatest error. The final model is a weighted sum of all the individual trees, where each tree contributes to reducing the overall error.\nSo, for example, to train a boosted tree model with 2000 trees, we first need to train a boosted tree model with 1 tree. Then, we need to take that model, figure out where it made its largest errors, and train a second tree that aims to correct those errors. So on, until the, say, 667-th tree, and so on until the 1333-rd tree, and so on until, finally, the 2000-th tree. Picking up what I’m putting down? Along the way to training a boosted tree with 2000 trees, we happened to train a bunch of other models we might be interested in evaluating: what we call submodels. So, in the example of bt_grid_regular, for a given learn_rate, we only need to train the model with the maximum trees. In this example, that’s a quarter of the model fits.\n\n\n\n\n\n\nNote\n\n\n\nYou might note that we don’t see a speedup nearly as drastic as 4 times. While we indeed only fit a quarter of the models, we’re fitting the boosted trees with the largest number of trees, and the time to train a boosted tree scales linearly with the number of trees. Said another way, we’re eliminating the need to fit only the faster-fitting models. This tends to be the case in many cases where the submodel trick applies.\n\n\nTo evaluate a fitted model with performance metrics, all we need are its predictions (and, usually, the true values being predicted). In pseudocode, resampling a model against performance metrics usually goes something like this:\n\nfor (resample in resamples) {\n  # analogue to the \"training\" set for the resample\n  analysis &lt;- analysis(resample)\n  # analogue to the \"testing\" set for the resample\n  assessment &lt;- assessment(resample)\n  \n  for (model in models) {\n    # the longest-running operation:\n    model_fit &lt;- fit(model, analysis)\n    \n    # usually, comparatively quick operations:\n    model_predictions &lt;- predict(model_fit, assessment)\n    metrics &lt;- c(metrics, metric(model_predictions))\n  }\n}\n\n\n\n\n\n\n\nNote\n\n\n\nanalysis(), assessment(), fit(), and predict() are indeed actual functions in tidymodels. metric() is not, but an analogue could be created from the output of the function metric_set().\n\n\nAmong all of these operations, fitting the model with fit() is usually the longest-running step, by far. In comparison, predict()ing on new values and calculating metrics takes very little time. Using the submodel trick allows us to reduce the number of fit()s while keeping the number of calls to predict() and metric() constant. In pseudocode, resampling with the submodel trick could look like:\n\nfor (resample in resamples) {\n  analysis &lt;- analysis(resample)\n  assessment &lt;- assessment(resample)\n  \n  models_to_fit &lt;- models[unique(non_submodel_args)]\n  \n  for (model in models_to_fit) {\n    model_fit &lt;- fit(model, analysis)\n    \n    for (model_to_eval in models[unique(submodel_args)]) {\n      model_to_eval &lt;- predict(model_to_eval, assessment)\n      metrics &lt;- c(metrics, metric(model_predictions))\n    }\n  }\n}\n\nThe above pseudocode admittedly requires some generosity (or mental gymnastics) to interpret, but the idea is that if fit()ting is indeed the majority of the time spent in resampling, and models_to_fit contains many fewer elements than models in the preceding pseudocode blocks, we should see substantial speedups.\n\n\n5.1.3 At its most extreme\nIn the applied example above, we saw a relatively modest speedup. If we want to really show off the power of the submodel trick, in terms of time spent resampling per model evaluated, we can come up with a somewhat silly grid:\n\nbt_grid_regular_go_brrr &lt;-\n  bt_grid_regular %&gt;%\n  slice_max(trees, by = learn_rate) %&gt;%\n  map(.x = c(1, seq(10, max(.$trees), 10)), .f = ~mutate(.y, trees = .x), dfr = .) %&gt;%\n  bind_rows()\n\nbt_grid_regular_go_brrr\n\n# A tibble: 804 × 2\n   trees learn_rate\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1    0.001  \n 2     1    0.00681\n 3     1    0.0464 \n 4     1    0.316  \n 5    10    0.001  \n 6    10    0.00681\n 7    10    0.0464 \n 8    10    0.316  \n 9    20    0.001  \n10    20    0.00681\n# ℹ 794 more rows\n\n\nIn this grid, we have the same number of unique values of learn_rate, \\(4\\), resulting in the same \\(4\\) model fits. Except that, in this case, we’re evaluating every model with number of trees \\(1, 10, 20, 30, 40, ..., 2000\\). If our hypothesis that predicting on the assessment set and generating performance metrics is comparatively fast is true, then we’ll see that elapsed time per grid point is way lower:\n\nset.seed(1)\n\nbm_grid_regular_go_brrr &lt;- \n  bench::mark(\n    grid_regular_go_brrr = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid_regular_go_brrr\n      )\n  )\n\nThe above code took 1.44 hours to run, a bit longer than the 1.23 hours from bm_grid_regular (that fitted the same number of models), but comparable. Per grid point, that difference is huge:\n\n# time per grid point for bm_grid_regular\nbm_grid_regular$median[[1]] / nrow(bt_grid_regular)\n\n[1] 4.62m\n\n# time per grid point for bm_grid_regular_go_brrr\nbm_grid_regular_go_brrr$median[[1]] / nrow(bt_grid_regular_go_brrr)\n\n[1] 6.44s\n\n\nWhether this difference in timing is of practical significance to a user is debatable. In the context where we generated bm_grid_regular, where the grid of points searched over is relatively comparable (and thus similarly likely to identify a performant model) yet the decreased number of model fits gave rise to a reasonable speedup—-99.67%—is undoubtedly impactful for many typical use cases of tidymodels. The more eye-popping per-grid-point speedups, as with bm_grid_regular_go_brrr, are more so a fun trick than a practical tool for most use cases.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "05-submodel.html#overview",
    "href": "05-submodel.html#overview",
    "title": "5  The submodel trick",
    "section": "5.2 Overview",
    "text": "5.2 Overview\nI’ve demonstrated the impact of the submodel trick in the above section through one example context, tuning trees in an XGBoost boosted tree model. The submodel trick can be found in many places across the tidymodels framework, though.\nSubmodels are defined with respect to a given model parameter, e.g. trees in boost_tree(). While a given parameter often defines a submodel regardless of modeling engine for a given model type—e.g. trees defines submodels for boost_tree() models regardless of whether the model is fitted with engine = \"xgboost\" or engine = \"lightgbm\"—there are some exceptions. Many tidymodels users undoubtedly tune models using arguments that define submodels without even knowing it. Some common examples include:\n\npenalty in linear_reg(), logistic_reg(), and multinom_reg(), which controls the amount of regularization in linear models.\nneighbors in nearest_neighbor(), the number of training data points nearest the point to be predicted that are factored into the prediction.\n\nAgain, some modeling engines for each of these model types do not actually support prediction from submodels from the noted parameter. See Section 5.2.1 for a complete table of currently supported arguments defining submodels in tidymodels.\nIn the above example, we had to manually specify a grid like bt_regular_grid in order for tidymodels to use a grid that can take advantage of the submodel trick. This reflects the frameworks’ general prioritization of predictive performance over computational performance in its design and defaults; latin hypercubes have better statistical properties when it comes to discovering performant hyperparameter combinations than a regular grid (McKay, Beckman, and Conover 2000; Stein 1987; Santner et al. 2003). However, note that in the case where a model is being tuned over only one argument, the submodel trick will kick in regardless of the sampling approach being used: regardless of how a set of univariate points are distributed, the most extreme parameter value (e.g. the max trees) can be used to generate predictions for values across the distribution.\n\n5.2.1 Supported parameters\nA number of tuning parameters support the submodel trick:\n\n\n\n\n\nModel Type\nArgument\nEngines\n\n\n\n\nboost_tree\ntrees\nxgboost, C5.0, lightgbm\n\n\nC5_rules\ntrees\nC5.0\n\n\ncubist_rules\nneighbors\nCubist\n\n\ndiscrim_flexible\nnum_terms\nearth\n\n\nlinear_reg\npenalty\nglmnet\n\n\nlogistic_reg\npenalty\nglmnet\n\n\nmars\nnum_terms\nearth\n\n\nmultinom_reg\npenalty\nglmnet\n\n\nnearest_neighbor\nneighbors\nkknn\n\n\npls\nnum_comp\nmixOmics\n\n\npoisson_reg\npenalty\nglmnet\n\n\nproportional_hazards\npenalty\nglmnet\n\n\nrule_fit\npenalty\nxrf\n\n\n\n\n\n\n\n\n\nDupuy, Delphine, Céline Helbert, and Jessica Franco. 2015. “DiceDesign and DiceEval: Two r Packages for Design and Analysis of Computer Experiments.” Journal of Statistical Software 65 (11).\n\n\nMcKay, Michael D, Richard J Beckman, and William J Conover. 2000. “A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.” Technometrics 42 (1): 55–61.\n\n\nSantner, Thomas J, Brian J Williams, William I Notz, and Brain J Williams. 2003. The Design and Analysis of Computer Experiments. Vol. 1. Springer.\n\n\nStein, Michael. 1987. “Large Sample Properties of Simulations Using Latin Hypercube Sampling.” Technometrics 29 (2): 143–51.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "06-extras.html",
    "href": "06-extras.html",
    "title": "6  Extras",
    "section": "",
    "text": "6.1 Preprocessing",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "06-extras.html#sec-gpu-training",
    "href": "06-extras.html#sec-gpu-training",
    "title": "6  Extras",
    "section": "6.3 GPU Training",
    "text": "6.3 GPU Training",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "06-extras.html#sec-stacking",
    "href": "06-extras.html#sec-stacking",
    "title": "6  Extras",
    "section": "6.4 Stacking",
    "text": "6.4 Stacking",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bengtsson, Henrik. 2021. “A Unifying\nFramework for Parallel and Distributed Processing in R using\nFutures.” The R Journal 13 (2):\n273–91. https://doi.org/10.32614/RJ-2021-048.\n\n\nDupuy, Delphine, Céline Helbert, and Jessica Franco. 2015.\n“DiceDesign and DiceEval: Two r Packages for Design and Analysis\nof Computer Experiments.” Journal of Statistical\nSoftware 65 (11).\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with\nR. \" O’Reilly Media, Inc.\".\n\n\nMcKay, Michael D, Richard J Beckman, and William J Conover. 2000.\n“A Comparison of Three Methods for Selecting Values of Input\nVariables in the Analysis of Output from a Computer Code.”\nTechnometrics 42 (1): 55–61.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSantner, Thomas J, Brian J Williams, William I Notz, and Brain J\nWilliams. 2003. The Design and Analysis of Computer\nExperiments. Vol. 1. Springer.\n\n\nStein, Michael. 1987. “Large Sample Properties of Simulations\nUsing Latin Hypercube Sampling.” Technometrics 29 (2):\n143–51.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Efficient Machine Learning with R",
    "section": "",
    "text": "I’m glad you’re here.\nWelcome to Efficient Machine Learning with R! This is a book about predictive modeling with tidymodels, focused on reducing the time and memory required to train machine learning models without sacrificing predictive performance.",
    "crumbs": [
      "I'm glad you're here."
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Efficient Machine Learning with R",
    "section": "Outline:",
    "text": "Outline:\n\nThe Introduction demonstrates a 145-fold speedup with an applied example. By adapting a grid search on a canonical model to use a more performant modeling engine, hooking into a parallel computing framework, transitioning to an optimized search strategy, and defining the grid to search over carefully, the section shows that users can drastically cut down on tuning times without sacrificing predictive performance. The following chapters then explore those optimizations in further details.\nThe Models chapter explore timings to resample various different modeling engines. The chapter compares implementations both within and across model types.\n\nParallelism compares various approaches to distributing model computations across CPU cores. We’ll explore two different across-model parallel computing frameworks supported by tidymodels—process forking and socket clusters—and explore their relationship to within-model parallelization.\nThen, Search explores various alternatives to grid search that can reduce the total number of model fits required to search a given grid space by only resampling models that seem to have a chance at being the “best.”\nFinally, Submodels investigates approaches to designing grids that can further reduce the total number of model fits required to search a given grid space by generating predictions from one model that can be used to evaluate several.\n\nThe optimizations discussed in those aforementioned chapters can, on their own, substantially reduce the time to evaluate machine learning models with tidymodels. Depending on the problem context, some modeling workflows may benefit from more specialized optimizations. The following chapters discuss some of those use cases:\n\nPreprocessing\nSparsity\nGPU Training\nStacking\n\n\n\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "I'm glad you're here."
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Early value\nTo demonstrate the impact that a couple small pieces of intuition can have on execution time when evaluating machine learning models, I’ll run through a quick model tuning example. On the first go, I’ll lean on tidymodels’ default values and a simple grid search, and on the second, I’ll pull a few tricks out from my sleeves that will drastically reduce the time to evaluate models while only negligibly decreasing predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#early-value",
    "href": "01-intro.html#early-value",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Setup\nFirst, loading a few needed packages:\n\nlibrary(tidymodels)\nlibrary(future)\nlibrary(finetune)\nlibrary(bonsai)\n\nFor the purposes of this example, we’ll simulate a data set of 100,000 rows and 18 columns. The first column, class, is a binary outcome, and the remaining variables are a mix of numerics and factors.\n\nset.seed(1)\nd &lt;- simulate_classification(1e5)\n\nWe’ll first split the data into training and testing sets before generating a set of 10 folds from the training data for cross-validation.\n\nset.seed(1)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\nd_folds &lt;- vfold_cv(d_train)\n\n\n\n1.1.2 A first go\nFor my first go at tuning, I’ll tune a boosted tree model using grid search. By default, tidymodels will use XGBoost as the modeling engine; I’ll try out a few different values for learn_rate— a parameter that controls how drastically newly added trees impact predictions—and trees—the number of trees in the ensemble.\n\nbt &lt;- \n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")\n\nI’ll carry out a grid search using tune_grid(), trying out a bunch of different pairs of values for learn_rate and trees and seeing what sticks. The argument grid = 12 indicates that I want to try out 12 different combinations of values and will let tidymodels take care of exactly what those values are.\n\nset.seed(1)\n\nbm_basic &lt;- \n  bench::mark(\n    basic = \n      tune_grid(\n        object = bt,\n        preprocessor = class ~ .,\n        resamples = d_folds,\n        grid = 12\n      )\n  )\n\nbench::mark() returns, among other things, a precise timing of how long this process takes.\n\nbm_basic\n\n# A tibble: 1 × 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 basic         3.68h    5.43GB\n\n\n\nHoly smokes! 3.68 hours is a good while. What all did tune_grid() do, though?\nFirst, let’s break down how many model fits actually happened. Since I’ve supplied grid = 12, we’re evaluating 12 possible model configurations. Each of those model configurations is evaluated against d_folds, a 10-fold cross validation object, meaning that each configuration is fitted 10 times. That’s 120 model fits!\nFurther, consider that those fits happen on 9/10ths of the training data, or 67500 rows.\n\nWith a couple small changes, though, the time to tune this model can be drastically decreased.\n\n\n1.1.3 A speedy go\nTo cut down on the time to evaluate these models, I’ll make a few small modifications.\nFirst, I’ll evaluate in parallel: Almost all modern laptops have more than one CPU core, and distributing computations across them only takes a couple lines of code with tidymodels.\n\nplan(multisession, workers = 4)\n\nWhile this tuning process could benefit from distributing across many more cores than 4, I’ll just use 4 here to give a realistic picture of the kinds of speedups possible on a typical laptop.\nThen, we’ll use a clever grid: The tidymodels framework enables something called the “submodel trick,” a technique that will allow us to predict from many more models than we actually fit. Instead of just supplying grid = 12, I’ll construct the grid myself.\n\nset.seed(1)\nbt_grid &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about the submodel trick, see Chapter 5.\n\n\nNext, I’ll switch out the computational engine: Substituting XGBoost with another gradient-boosting model that can better handle some properties of this dataset will cut down on our fit time by a good bit.\n\nbt_lgb &lt;- bt %&gt;% set_engine(\"lightgbm\")\n\nFinally, I’ll give up early on poorly-performing models: Rather than using grid search with tune_grid(), I’ll use a technique called racing that stops evaluating models when they seem to be performing poorly using the tune_race_anova() function.\n\nset.seed(1)\n\nbm_speedy &lt;- \n  bench::mark(\n    speedy = \n      tune_race_anova(\n        object = bt_lgb,\n        preprocessor = class ~ .,\n        resamples = d_folds,\n        grid = bt_grid\n      )\n  )\n\nChecking out the new benchmarks:\n\nbm_speedy\n\n# A tibble: 1 × 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 speedy        1.52m    47.5MB\n\n\nThe total time to tune was reduced from 3.68 hours to 1.52 minutes—the second approach was 145 times faster than the first.\nThe first thing I’d wonder when seeing this result is how much of a penalty in predictive performance I’d suffer due to this transition. Let’s evaluate both of the top models from these tuning results on the test set. First, for the basic workflow:\n\nfit_basic &lt;- \n  select_best(bm_basic$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(class ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = d_split)\n\n\ncollect_metrics(fit_basic)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.835 Preprocessor1_Model1\n2 roc_auc     binary         0.896 Preprocessor1_Model1\n3 brier_class binary         0.119 Preprocessor1_Model1\n\n\nAs for the quicker approach:\n\nfit_speedy &lt;- \n  select_best(bm_speedy$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(class ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = d_split)\n\n\ncollect_metrics(fit_speedy)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.835 Preprocessor1_Model1\n2 roc_auc     binary         0.895 Preprocessor1_Model1\n3 brier_class binary         0.120 Preprocessor1_Model1\n\n\nVirtually indistinguishable performance results in 0.7% of the time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#our-approach",
    "href": "01-intro.html#our-approach",
    "title": "1  Introduction",
    "section": "1.2 Our approach",
    "text": "1.2 Our approach\nThis book is intended for tidymodels users who have been waiting too long for their code to run. I generally assume that users are familiar with data manipulation and visualization with the tidyverse as well as the basics of machine learning with tidymodels, like evaluating models against resamples using performance metrics. For the former, I recommend (Wickham, Çetinkaya-Rundel, and Grolemund 2023) for getting up to speed—for the latter, (Kuhn and Silge 2022). If you’re generally comfortable with the content in those books, you’re ready to go.\nModern laptops are remarkable. Users of tidymodels working on many machines made in the last few years are well-prepared to interactively develop machine learning models based on tens of millions of rows of data. That said, without the right information, it’s quite easy to mistakenly introduce performance issues that result in analyses on even tens of thousands of rows of data becoming too cumbersome to work with. Generally, the tidymodels framework attempts to guard users from making such mistakes and addressing them ourselves when they’re in our control. At the same time, many foundational and well-used approaches in classical machine learning have well-theorized adaptations that substantially cut down on the elapsed time while preserving predictive performance. The tidymodels framework implements many such adaptations and this book aims to surface them in a holistic and coherent way. Readers will come out of having read this book with a grab bag of one-liners that can cut down on elapsed time to develop machine learning models by orders of magnitude.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-cost-of-slowness",
    "href": "01-intro.html#the-cost-of-slowness",
    "title": "1  Introduction",
    "section": "1.3 The cost of slowness",
    "text": "1.3 The cost of slowness\nAll of this said, R is not known for its computational efficiency. If I really prioritize that, why am I writing a book about R?\n(i use R for many reasons. you evidently do, too. it is true that many modeling engines only implement some performance optimizations / hardware accelators for their python interfaces—at the same time, modeling engines in R are often interfaces to the same compiled code as that used from python.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-hard-part",
    "href": "01-intro.html#the-hard-part",
    "title": "1  Introduction",
    "section": "1.4 The hard part",
    "text": "1.4 The hard part\nTo better understand how to cut down on the time to evaluate models with tidymodels, we need to understand a bit about how tidymodels works.\nLike many other “unifying frameworks” for ML (mlr3, caret, scikit-learn(?)), the tidymodels framework itself does not implement the algorithms to train and predict from models. Instead, tidymodels provides a common interface to modeling engines: packages (or functions from packages) that provide the methods to fit() and predict().\n\n\n\n\n\n\nFigure 1.1\n\n\n\nThe process of “translating” between the tidymodels and engine formats is illustrated in Figure 1.1. When fitting and predicting with tidymodels, some portion of the elapsed time to run code is due to the “translation” of the inputted unified code to the specific syntax that the engine expects, and some portion of it is due to the translation of what the engine returns to the unified output returned by tidymodels; these portions are in the tidymodels team’s control. The rest of the elapsed time occurs inside of the modeling engine’s code.\nThe portions of the elapsed time that are in the tidymodels team’s control are shown in green, and I’ll refer to them in this book as “overhead.” The overhead of tidymodels in terms of elapsed time is relatively constant with respect to the size of training data. This overhead consists of tasks like checking data types, handling errors and warnings, and—most importantly—programmatically assembling calls to engine functions.\nThe portion of the elapsed time shown in orange represents the actual training of (or predicting from) the model. This portion is implemented by the modeling engine and is thus not in the tidymodels team’s control. In contrast to overhead, the elapsed time of this code is very much sensitive to the size of the inputted data; depending on the engine, increases in the number of rows or columns of training or testing data may drastically increase the time to train or predict from a given model.\n\n\n\n\n\n\nNote\n\n\n\nThe algorithmic complexity of the models implemented by these engines is well-understood in many cases. At the same time, the behavior of elapsed time for some engine implementations often differs greatly from what theory would lead one to believe. Regressions in modeling code may introduce undue slowdowns and, conversely, performance optimizations that lead to elapsed times that scale better than theory would suggest may be the very reason for the existence of some engines.\n\n\nAs shown in Figure 1.2, the proportion of elapsed time that overhead is responsible for depends on how quickly the engine can fit or predict for a given dataset.\n\n\n\n\n\n\nFigure 1.2\n\n\n\nSince the absolute overhead of tidymodels’ translation is relatively constant, overhead is only a substantial portion of elapsed time when models fit or predict very quickly. For a linear model fitted on 30 data points with lm(), this overhead is continuously benchmarked to remain under 2/3. That is, absolute worst-case, fitting a model with tidymodels takes three times longer than using the engine interface itself. However, this overhead approaches fractions of a percent for fits on even 10,000 rows for many engines. Thus, a focus on reducing the elapsed time of overhead is valuable in the sense that the framework ought not to unintentionally introduce regressions that cause overhead to scale with the size of training data, but in general, the hard part of reducing elapsed time when evaluating models is reducing the elapsed time for computations carried out by the modeling engine.\nThe next question is then how could tidymodels cut down on elapsed time for modeling engines that it doesn’t own? To answer this question, let’s revisit the applied example from Section 1.1.2. In that first example, the code does some translation to the engine’s syntax, sets up some error handling, and then fits and predicts from 120 models.\n\n\n\n\n\n\nFigure 1.3\n\n\n\nFigure 1.3 depicts this process, where we evaluate all `120 models in order. Each white dot in the engine portion of the elapsed time represents another round of fitting and predicting with engine. Remember that in reality, for even modest dataset sizes, the green portions representing tidymodels overhead are much smaller by proportion than represented.\nIn Section 1.1.3, the first thing I did was introduce a parallel backend. Distributing engine fits across available cores is itself a gamechanger, as illustrated in Figure 1.4.\n\n\n\n\n\n\nFigure 1.4\n\n\n\nThen, switching out computational engines for a more performant alternative further reduces elapsed time, as shown in ?fig-parallel-resample-opt.\n\nFinally, as depicted in ?fig-parallel-resample-opt2, the submodel trick described in Chapter 5 and racing described in Chapter 4 eliminate a substantial portion of the engine fits.\n\nThe tidymodels team devotes substantial energy to ensuring support for the most performant parallelization technologies, modeling engines, model-specific optimizations, and search techniques. This book will demonstrate how to best make use of these features to reduce the time needed to evaluate machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-datasets",
    "href": "01-intro.html#sec-datasets",
    "title": "1  Introduction",
    "section": "1.5 Datasets",
    "text": "1.5 Datasets\nIn Section 1.1.1, I used a function simulate_classification() to generate data. This is one of two functions, the other being simulate_regression(), that create the data underlying many of the experiments in this book.\nThese two functions are adaptations of their similarly named friends sim_classification() and sim_regression() from the modeldata package. They make small changes to those function—namely, introducing factor predictors with some tricky distributions—that surface slowdowns with some modeling engines.\nProvided a number of rows, sim_classification() generates a tibble with that many rows and 16 columns:\n\nd_class &lt;- simulate_classification(1000)\n\nd_class\n\n# A tibble: 1,000 × 18\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;   &lt;fct&gt;               &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_1 level_2          -0.133   level_3            0.0355       0.770 \n 2 class_1 level_2           0.894   level_3            0.356        0.690 \n 3 class_2 level_2          -1.59    level_4            0.249        0.650 \n 4 class_1 level_1           2.17    level_3            0.879        0.0747\n 5 class_1 level_2           0.464   level_1            0.318        0.903 \n 6 class_2 level_2          -2.04    level_3            0.321        0.133 \n 7 class_2 level_2           1.11    level_3            0.848        0.211 \n 8 class_2 level_1          -0.183   level_3            0.381        0.155 \n 9 class_2 level_1           0.00202 level_3            0.275        0.0545\n10 class_2 level_2           0.198   level_3            0.918        0.715 \n# ℹ 990 more rows\n# ℹ 12 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;fct&gt;, linear_11 &lt;fct&gt;,\n#   linear_12 &lt;fct&gt;\n\n\nThe leftmost column, class, is a binary outcome variable, and the remaining columns are predictor variables. The predictors throw a few curveballs at the modeling functions we’ll benchmark in this book.\nFor one, the predictors are moderately correlated. Correlated predictors can lead to unstable parameter estimates and can make the model more sensitive to small changes in the data. Further, correlated predictors may lead to slower convergence in gradient descent processes (like those driving gradient-boosted trees like XGBoost and LightGBM), as the resulting elongated and narrow surface of loss functions causes the algorithm to zigzag towards the optimum value, significantly increasing the training time along the way.\n\n\n\n\n\n\n\n\n\nSecondly, there are a number of factor predictors. Some modeling engines experience slower training times with many factor predictors for a variety of reasons. For one, most modeling engines ultimately implement training routines on numeric matrices, requiring that factor predictors are somehow encoded as numbers. Most often in R, this is in the form of treatment contrasts, where an \\(n\\)-length factor with \\(l\\) levels is represented as an \\(n ~x~ l-1\\) matrix composed of zeroes and ones. Each column is referred to as a dummy variable. The first column has value \\(1\\) when the \\(i\\)-th entry of the factor is the second level, zero otherwise. The second column has value \\(1\\) when the \\(i\\)-th entry of the factor is the third level, zero otherwise. We know that the \\(i\\)-th entry of the first took its first level if all of the entries in the \\(i\\)th column of the resulting matrix are zero. While this representation of a factor is relatively straightforward, it’s quite memory intensive; a factor with 100 levels ultimately will require a 99-column matrix to be allocated in order to be included in a model. While many modeling engines in R assume that factors will be encoded as treatment constrasts, different modeling engines have different approaches to processing factor variables, some more efficient than others. More on this in Section 6.2, in particular.\n\n\nOriginal factor\n\n\n  x\n1 a\n2 b\n3 c\n\n\n\n\n\nFactor with treatment contrasts\n\n\n  xb xc\n1  0  0\n2  1  0\n3  0  1\n\n\n\n\nFurther, many of those factor variables have a class imbalance; that is, some levels of the factor occur much more often than others. Some models may struggle to learn from the less frequently-occurring classes, potentially requiring more iterations of descent processes for some models to converge. Even when this is not the case, it may be varyingly “worth it” in terms of memory usage to allocate a dummy variable to a factor level that only appears a couple times in a dataset with many rows.\n\n\n\n\n\n\n\n\n\nThe regression dataset looks quite similar.\n\nd_reg &lt;- simulate_regression(1000)\n\nd_reg\n\n# A tibble: 1,000 × 16\n   outcome predictor_01 predictor_02 predictor_03 predictor_04 predictor_05\n     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1    6.36       -2.45         2.46          2.85        2.16        -1.28 \n 2   -2.11       -4.08         1.64         -3.49        3.21        -3.18 \n 3   26.0        -4.95         1.77         -2.21        3.35         5.29 \n 4   -1.31       -3.36        -2.95         -4.05       -0.911        5.88 \n 5   53.8         0.604       -6.35         -4.89       -5.96        -7.86 \n 6  -32.1         3.53        -6.56          2.06       -0.965       -2.11 \n 7   24.8         2.59         1.68         -1.88        3.10        -1.35 \n 8   11.7         3.79        -1.46         -2.23       -0.467       -0.662\n 9    7.19        4.88         1.08          4.17        3.01        -4.15 \n10    9.43        2.78        -0.441         1.62        0.452        1.74 \n# ℹ 990 more rows\n# ℹ 10 more variables: predictor_06 &lt;dbl&gt;, predictor_07 &lt;dbl&gt;,\n#   predictor_08 &lt;dbl&gt;, predictor_09 &lt;dbl&gt;, predictor_10 &lt;fct&gt;,\n#   predictor_11 &lt;fct&gt;, predictor_12 &lt;fct&gt;, predictor_13 &lt;fct&gt;,\n#   predictor_14 &lt;fct&gt;, predictor_15 &lt;fct&gt;\n\n\nThe left-most column, outcome, is a numeric outcome, and the remaining 15 columns are a mix of numeric and factor. The same story related to correlation and tricky factor imbalances goes for the regression dataset. Demonstrating that is homework.\n\n\n\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R. \" O’Reilly Media, Inc.\".\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "06-extras.html#sec-sparsity",
    "href": "06-extras.html#sec-sparsity",
    "title": "6  Extras",
    "section": "6.2 Sparsity",
    "text": "6.2 Sparsity",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Extras</span>"
    ]
  }
]