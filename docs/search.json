[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Efficient Machine Learning with R",
    "section": "",
    "text": "Welcomeüêõ\nWelcome to Efficient Machine Learning with R! This is a book about predictive modeling with tidymodels, focused on reducing the time and memory required to train machine learning models without sacrificing predictive performance.\nThis book assumes familiarity with data analysis with the tidyverse as well as the basics of machine learning with tidymodels: fitting models with parsnip, resampling data with rsample, and tuning model parameters with tune. For more on tidy data analysis, see (Wickham, √áetinkaya-Rundel, and Grolemund 2023). For the basics of predictive modeling with tidymodels, see (Kuhn and Silge 2022).",
    "crumbs": [
      "Welcomeüêõ"
    ]
  },
  {
    "objectID": "index.html#outline",
    "href": "index.html#outline",
    "title": "Efficient Machine Learning with R",
    "section": "Outline:",
    "text": "Outline:\n\nThe Introduction demonstrates a 145-fold speedup with an applied example. By adapting a grid search on a canonical model to use a more performant modeling engine, hooking into a parallel computing framework, transitioning to an optimized search strategy, and defining the grid to search over carefully, the section shows that users can drastically cut down on tuning times without sacrificing predictive performance. The following chapters then explore those optimizations in further details.\nThe Models chapter explore timings to resample various different modeling engines. The chapter compares implementations both within and across model types.\n\nParallelism compares various approaches to distributing model computations across CPU cores. We‚Äôll explore two different across-model parallel computing frameworks supported by tidymodels‚Äîprocess forking and socket clusters‚Äîand explore their relationship to within-model parallelization.\nThen, Search explores various alternatives to grid search that can reduce the total number of model fits required to search a given grid space by only resampling models that seem to have a chance at being the ‚Äúbest.‚Äù\nFinally, Submodels investigates approaches to designing grids that can further reduce the total number of model fits required to search a given grid space by generating predictions from one model that can be used to evaluate several.\n\nThe optimizations discussed in those aforementioned chapters can, on their own, substantially reduce the time to evaluate machine learning models with tidymodels. Depending on the problem context, some modeling workflows may benefit from more specialized optimizations. The following chapters discuss some of those use cases:\n\nPreprocessing\nSparsity\nGPU Training\nStacking\n\n\n\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O‚ÄôReilly Media, Inc.\".",
    "crumbs": [
      "Welcomeüêõ"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 Early value\nTo demonstrate the impact that a couple small pieces of intuition can have on execution time when evaluating machine learning models, I‚Äôll run through a quick model tuning example. On the first go, I‚Äôll lean on tidymodels‚Äô default values and a simple grid search, and on the second, I‚Äôll pull a few tricks out from my sleeves that will drastically reduce the time to evaluate models while only negligibly decreasing predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#early-value",
    "href": "01-intro.html#early-value",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1.1 Setup\nFirst, loading a few needed packages:\n\nlibrary(tidymodels)\nlibrary(future)\nlibrary(finetune)\nlibrary(bonsai)\n\nFor the purposes of this example, we‚Äôll simulate a data set of 100,000 rows and 18 columns. The first column, class, is a binary outcome, and the remaining variables are a mix of numerics and factors.\n\nset.seed(1)\nd &lt;- simulate_classification(1e5)\n\nWe‚Äôll first split the data into training and testing sets before generating a set of 10 folds from the training data for cross-validation.\n\nset.seed(1)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\nd_folds &lt;- vfold_cv(d_train)\n\n\n\n1.1.2 A first go\nFor my first go at tuning, I‚Äôll tune a boosted tree model using grid search. By default, tidymodels will use XGBoost as the modeling engine; I‚Äôll try out a few different values for learn_rate‚Äî a parameter that controls how drastically newly added trees impact predictions‚Äîand trees‚Äîthe number of trees in the ensemble.\n\nbt &lt;- \n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")\n\nI‚Äôll carry out a grid search using tune_grid(), trying out a bunch of different pairs of values for learn_rate and trees and seeing what sticks. The argument grid = 12 indicates that I want to try out 12 different combinations of values and will let tidymodels take care of exactly what those values are.\n\nset.seed(1)\n\nbm_basic &lt;- \n  bench::mark(\n    basic = \n      tune_grid(\n        object = bt,\n        preprocessor = class ~ .,\n        resamples = d_folds,\n        grid = 12\n      )\n  )\n\nbench::mark() returns, among other things, a precise timing of how long this process takes.\n\nbm_basic\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 basic         3.68h    5.43GB\n\n\n\nHoly smokes! 3.68 hours is a good while. What all did tune_grid() do, though?\nFirst, let‚Äôs break down how many model fits actually happened. Since I‚Äôve supplied grid = 12, we‚Äôre evaluating 12 possible model configurations. Each of those model configurations is evaluated against d_folds, a 10-fold cross validation object, meaning that each configuration is fitted 10 times. That‚Äôs 120 model fits!\nFurther, consider that those fits happen on 9/10ths of the training data, or 67500 rows.\n\nWith a couple small changes, though, the time to tune this model can be drastically decreased.\n\n\n1.1.3 A speedy go\nTo cut down on the time to evaluate these models, I‚Äôll make a few small modifications.\nFirst, I‚Äôll evaluate in parallel: Almost all modern laptops have more than one CPU core, and distributing computations across them only takes a couple lines of code with tidymodels.\n\nplan(multisession, workers = 4)\n\nWhile this tuning process could benefit from distributing across many more cores than 4, I‚Äôll just use 4 here to give a realistic picture of the kinds of speedups possible on a typical laptop.\nThen, we‚Äôll use a clever grid: The tidymodels framework enables something called the ‚Äúsubmodel trick,‚Äù a technique that will allow us to predict from many more models than we actually fit. Instead of just supplying grid = 12, I‚Äôll construct the grid myself.\n\nset.seed(1)\nbt_grid &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about the submodel trick, see Chapter 5.\n\n\nNext, I‚Äôll switch out the computational engine: Substituting XGBoost with another gradient-boosting model that can better handle some properties of this dataset will cut down on our fit time by a good bit.\n\nbt_lgb &lt;- bt %&gt;% set_engine(\"lightgbm\")\n\nFinally, I‚Äôll give up early on poorly-performing models: Rather than using grid search with tune_grid(), I‚Äôll use a technique called racing that stops evaluating models when they seem to be performing poorly using the tune_race_anova() function.\n\nset.seed(1)\n\nbm_speedy &lt;- \n  bench::mark(\n    speedy = \n      tune_race_anova(\n        object = bt_lgb,\n        preprocessor = class ~ .,\n        resamples = d_folds,\n        grid = bt_grid\n      )\n  )\n\nChecking out the new benchmarks:\n\nbm_speedy\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 speedy        1.52m    47.5MB\n\n\nThe total time to tune was reduced from 3.68 hours to 1.52 minutes‚Äîthe second approach was 145 times faster than the first.\nThe first thing I‚Äôd wonder when seeing this result is how much of a penalty in predictive performance I‚Äôd suffer due to this transition. Let‚Äôs evaluate both of the top models from these tuning results on the test set. First, for the basic workflow:\n\nfit_basic &lt;- \n  select_best(bm_basic$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(class ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = d_split)\n\n\ncollect_metrics(fit_basic)\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.835 Preprocessor1_Model1\n2 roc_auc     binary         0.896 Preprocessor1_Model1\n3 brier_class binary         0.119 Preprocessor1_Model1\n\n\nAs for the quicker approach:\n\nfit_speedy &lt;- \n  select_best(bm_speedy$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(class ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = d_split)\n\n\ncollect_metrics(fit_speedy)\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.835 Preprocessor1_Model1\n2 roc_auc     binary         0.895 Preprocessor1_Model1\n3 brier_class binary         0.120 Preprocessor1_Model1\n\n\nVirtually indistinguishable performance results in 0.7% of the time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#our-approach",
    "href": "01-intro.html#our-approach",
    "title": "1¬† Introduction",
    "section": "1.2 Our approach",
    "text": "1.2 Our approach\nThis book is intended for tidymodels users who have been waiting too long for their code to run. I generally assume that users are familiar with data manipulation and visualization with the tidyverse as well as the basics of machine learning with tidymodels, like evaluating models against resamples using performance metrics. For the former, I recommend (Wickham, √áetinkaya-Rundel, and Grolemund 2023) for getting up to speed‚Äîfor the latter, (Kuhn and Silge 2022). If you‚Äôre generally comfortable with the content in those books, you‚Äôre ready to go.\nModern laptops are remarkable. Users of tidymodels working on many machines made in the last few years are well-prepared to interactively develop machine learning models based on tens of millions of rows of data. That said, without the right information, it‚Äôs quite easy to mistakenly introduce performance issues that result in analyses on even tens of thousands of rows of data becoming too cumbersome to work with. Generally, the tidymodels framework attempts to guard users from making such mistakes and addressing them ourselves when they‚Äôre in our control. At the same time, many foundational and well-used approaches in classical machine learning have well-theorized adaptations that substantially cut down on the elapsed time while preserving predictive performance. The tidymodels framework implements many such adaptations and this book aims to surface them in a holistic and coherent way. Readers will come out of having read this book with a grab bag of one-liners that can cut down on elapsed time to develop machine learning models by orders of magnitude.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-cost-of-slowness",
    "href": "01-intro.html#the-cost-of-slowness",
    "title": "1¬† Introduction",
    "section": "1.3 The cost of slowness",
    "text": "1.3 The cost of slowness\nAll of this said, R is not known for its computational efficiency. If I really prioritize that, why am I writing a book about R?\n(i use R for many reasons. you evidently do, too. it is true that many modeling engines only implement some performance optimizations / hardware accelators for their python interfaces‚Äîat the same time, modeling engines in R are often interfaces to the same compiled code as that used from python.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#the-hard-part",
    "href": "01-intro.html#the-hard-part",
    "title": "1¬† Introduction",
    "section": "1.3 The hard part",
    "text": "1.3 The hard part\nTo better understand how to cut down on the time to evaluate models with tidymodels, we need to understand a bit about how tidymodels works.\nLike many other ‚Äúunifying frameworks‚Äù for ML (mlr3, caret, scikit-learn(?)), the tidymodels framework itself does not implement the algorithms to train and predict from models. Instead, tidymodels provides a common interface to modeling engines: packages (or functions from packages) that provide the methods to fit() and predict().\n\n\n\n\n\n\nFigure¬†1.1\n\n\n\nThe process of ‚Äútranslating‚Äù between the tidymodels and engine formats is illustrated in Figure¬†1.1. When fitting and predicting with tidymodels, some portion of the elapsed time to run code is due to the ‚Äútranslation‚Äù of the inputted unified code to the specific syntax that the engine expects, and some portion of it is due to the translation of what the engine returns to the unified output returned by tidymodels; these portions are in the tidymodels team‚Äôs control. The rest of the elapsed time occurs inside of the modeling engine‚Äôs code.\nThe portions of the elapsed time that are in the tidymodels team‚Äôs control are shown in green, and I‚Äôll refer to them in this book as ‚Äúoverhead.‚Äù The overhead of tidymodels in terms of elapsed time is relatively constant with respect to the size of training data. This overhead consists of tasks like checking data types, handling errors and warnings, and‚Äîmost importantly‚Äîprogrammatically assembling calls to engine functions.\nThe portion of the elapsed time shown in orange represents the actual training of (or predicting from) the model. This portion is implemented by the modeling engine and is thus not in the tidymodels team‚Äôs control. In contrast to overhead, the elapsed time of this code is very much sensitive to the size of the inputted data; depending on the engine, increases in the number of rows or columns of training or testing data may drastically increase the time to train or predict from a given model.\n\n\n\n\n\n\nNote\n\n\n\nThe algorithmic complexity of the models implemented by these engines is well-understood in many cases. At the same time, the behavior of elapsed time for some engine implementations often differs greatly from what theory would lead one to believe. Regressions in modeling code may introduce undue slowdowns and, conversely, performance optimizations that lead to elapsed times that scale better than theory would suggest may be the very reason for the existence of some engines.\n\n\nAs shown in Figure¬†1.2, the proportion of elapsed time that overhead is responsible for depends on how quickly the engine can fit or predict for a given dataset.\n\n\n\n\n\n\nFigure¬†1.2\n\n\n\nSince the absolute overhead of tidymodels‚Äô translation is relatively constant, overhead is only a substantial portion of elapsed time when models fit or predict very quickly. For a linear model fitted on 30 data points with lm(), this overhead is continuously benchmarked to remain under 2/3. That is, absolute worst-case, fitting a model with tidymodels takes three times longer than using the engine interface itself. However, this overhead approaches fractions of a percent for fits on even 10,000 rows for many engines. Thus, a focus on reducing the elapsed time of overhead is valuable in the sense that the framework ought not to unintentionally introduce regressions that cause overhead to scale with the size of training data, but in general, the hard part of reducing elapsed time when evaluating models is reducing the elapsed time for computations carried out by the modeling engine.\nThe next question is then how could tidymodels cut down on elapsed time for modeling engines that it doesn‚Äôt own? To answer this question, let‚Äôs revisit the applied example from Section 1.1.2. In that first example, the code does some translation to the engine‚Äôs syntax, sets up some error handling, and then fits and predicts from 120 models.\n\n\n\n\n\n\nFigure¬†1.3\n\n\n\nFigure¬†1.3 depicts this process, where we evaluate all `120 models in order. Each white dot in the engine portion of the elapsed time represents another round of fitting and predicting with engine. Remember that in reality, for even modest dataset sizes, the green portions representing tidymodels overhead are much smaller by proportion than represented.\nIn Section 1.1.3, the first thing I did was introduce a parallel backend. Distributing engine fits across available cores is itself a gamechanger, as illustrated in Figure¬†1.4.\n\n\n\n\n\n\nFigure¬†1.4\n\n\n\nThen, switching out computational engines for a more performant alternative further reduces elapsed time, as shown in ?fig-parallel-resample-opt.\n\nFinally, as depicted in ?fig-parallel-resample-opt2, the submodel trick described in Chapter 5 and racing described in Chapter 4 eliminate a substantial portion of the engine fits.\n\nThe tidymodels team devotes substantial energy to ensuring support for the most performant parallelization technologies, modeling engines, model-specific optimizations, and search techniques. This book will demonstrate how to best make use of these features to reduce the time needed to evaluate machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#sec-datasets",
    "href": "01-intro.html#sec-datasets",
    "title": "1¬† Introduction",
    "section": "1.4 Datasets",
    "text": "1.4 Datasets\nIn Section 1.1.1, I used a function simulate_classification() to generate data. This is one of two functions, the other being simulate_regression(), that create the data underlying many of the experiments in this book.\nThese two functions are adaptations of their similarly named friends sim_classification() and sim_regression() from the modeldata package. They make small changes to those function‚Äînamely, introducing factor predictors with some tricky distributions‚Äîthat surface slowdowns with some modeling engines.\nProvided a number of rows, sim_classification() generates a tibble with that many rows and 16 columns:\n\nd_class &lt;- simulate_classification(1000)\n\nd_class\n\n# A tibble: 1,000 √ó 18\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;   &lt;fct&gt;               &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_1 level_2          -0.133   level_3            0.0355       0.770 \n 2 class_1 level_2           0.894   level_3            0.356        0.690 \n 3 class_2 level_2          -1.59    level_4            0.249        0.650 \n 4 class_1 level_1           2.17    level_3            0.879        0.0747\n 5 class_1 level_2           0.464   level_1            0.318        0.903 \n 6 class_2 level_2          -2.04    level_3            0.321        0.133 \n 7 class_2 level_2           1.11    level_3            0.848        0.211 \n 8 class_2 level_1          -0.183   level_3            0.381        0.155 \n 9 class_2 level_1           0.00202 level_3            0.275        0.0545\n10 class_2 level_2           0.198   level_3            0.918        0.715 \n# ‚Ñπ 990 more rows\n# ‚Ñπ 12 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;fct&gt;, linear_11 &lt;fct&gt;,\n#   linear_12 &lt;fct&gt;\n\n\nThe leftmost column, class, is a binary outcome variable, and the remaining columns are predictor variables. The predictors throw a few curveballs at the modeling functions we‚Äôll benchmark in this book.\nFor one, the predictors are moderately correlated. Correlated predictors can lead to unstable parameter estimates and can make the model more sensitive to small changes in the data. Further, correlated predictors may lead to slower convergence in gradient descent processes (like those driving gradient-boosted trees like XGBoost and LightGBM), as the resulting elongated and narrow surface of loss functions causes the algorithm to zigzag towards the optimum value, significantly increasing the training time along the way.\n\n\n\n\n\n\n\n\n\nSecondly, there are a number of factor predictors. Some modeling engines experience slower training times with many factor predictors for a variety of reasons. For one, most modeling engines ultimately implement training routines on numeric matrices, requiring that factor predictors are somehow encoded as numbers. Most often in R, this is in the form of treatment contrasts, where an \\(n\\)-length factor with \\(l\\) levels is represented as an \\(n ~x~ l-1\\) matrix composed of zeroes and ones. Each column is referred to as a dummy variable. The first column has value \\(1\\) when the \\(i\\)-th entry of the factor is the second level, zero otherwise. The second column has value \\(1\\) when the \\(i\\)-th entry of the factor is the third level, zero otherwise. We know that the \\(i\\)-th entry of the first took its first level if all of the entries in the \\(i\\)th column of the resulting matrix are zero. While this representation of a factor is relatively straightforward, it‚Äôs quite memory intensive; a factor with 100 levels ultimately will require a 99-column matrix to be allocated in order to be included in a model. While many modeling engines in R assume that factors will be encoded as treatment constrasts, different modeling engines have different approaches to processing factor variables, some more efficient than others. More on this in Section 6.2, in particular.\n\n\nOriginal factor\n\n\n  x\n1 a\n2 b\n3 c\n\n\n\n\n\nFactor with treatment contrasts\n\n\n  xb xc\n1  0  0\n2  1  0\n3  0  1\n\n\n\n\nFurther, many of those factor variables have a class imbalance; that is, some levels of the factor occur much more often than others. Some models may struggle to learn from the less frequently-occurring classes, potentially requiring more iterations of descent processes for some models to converge. Even when this is not the case, it may be varyingly ‚Äúworth it‚Äù in terms of memory usage to allocate a dummy variable to a factor level that only appears a couple times in a dataset with many rows.\n\n\n\n\n\nTODO: write caption\n\n\n\n\nThe regression dataset looks quite similar.\n\nd_reg &lt;- simulate_regression(1000)\n\nd_reg\n\n# A tibble: 1,000 √ó 16\n   outcome predictor_01 predictor_02 predictor_03 predictor_04 predictor_05\n     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1    6.36       -2.45         2.46          2.85        2.16        -1.28 \n 2   -2.11       -4.08         1.64         -3.49        3.21        -3.18 \n 3   26.0        -4.95         1.77         -2.21        3.35         5.29 \n 4   -1.31       -3.36        -2.95         -4.05       -0.911        5.88 \n 5   53.8         0.604       -6.35         -4.89       -5.96        -7.86 \n 6  -32.1         3.53        -6.56          2.06       -0.965       -2.11 \n 7   24.8         2.59         1.68         -1.88        3.10        -1.35 \n 8   11.7         3.79        -1.46         -2.23       -0.467       -0.662\n 9    7.19        4.88         1.08          4.17        3.01        -4.15 \n10    9.43        2.78        -0.441         1.62        0.452        1.74 \n# ‚Ñπ 990 more rows\n# ‚Ñπ 10 more variables: predictor_06 &lt;dbl&gt;, predictor_07 &lt;dbl&gt;,\n#   predictor_08 &lt;dbl&gt;, predictor_09 &lt;dbl&gt;, predictor_10 &lt;fct&gt;,\n#   predictor_11 &lt;fct&gt;, predictor_12 &lt;fct&gt;, predictor_13 &lt;fct&gt;,\n#   predictor_14 &lt;fct&gt;, predictor_15 &lt;fct&gt;\n\n\nThe left-most column, outcome, is a numeric outcome, and the remaining 15 columns are a mix of numeric and factor. The same story related to correlation and tricky factor imbalances goes for the regression dataset. Demonstrating that is homework.\n\n\n\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O‚ÄôReilly Media, Inc.\".",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-models.html",
    "href": "02-models.html",
    "title": "2¬† Models",
    "section": "",
    "text": "2.1 Tidymodels overhead\nWhile the tidymodels team develops the infrastructure that users interact with directly, under the hood, we send calls out to other people‚Äôs modeling packages‚Äîor modeling engines‚Äîthat provide the actual implementations that estimate parameters, generate predictions, etc. The process looks something like this:\nWhen thinking about the time allotted to each of the three steps above, we refer to the ‚Äútranslate‚Äù steps in green as the tidymodels overhead. The time it takes to ‚Äútranslate‚Äù interfaces in steps 1) and 3) is within our control, while the time the modeling engine takes to do it‚Äôs thing in step 2) is not.\nLet‚Äôs demonstrate with an example classification problem. Generating some random data:\nset.seed(1)\nd &lt;- simulate_classification(n_rows = 100)\n\nd\n\n# A tibble: 100 √ó 18\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;   &lt;fct&gt;               &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_2 level_1          -1.17    level_1             0.554        0.814\n 2 class_1 level_1           0.261   level_2             0.688        0.929\n 3 class_2 level_1          -1.61    level_1             0.658        0.147\n 4 class_1 level_1           2.14    level_1             0.663        0.750\n 5 class_2 level_1           0.0360  level_1             0.472        0.976\n 6 class_1 level_2          -0.00837 level_1             0.970        0.975\n 7 class_2 level_1           1.05    level_2             0.402        0.351\n 8 class_1 level_1           1.49    level_1             0.850        0.394\n 9 class_2 level_1           0.967   level_2             0.757        0.951\n10 class_2 level_2           0.603   level_1             0.533        0.107\n# ‚Ñπ 90 more rows\n# ‚Ñπ 12 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;fct&gt;, linear_11 &lt;fct&gt;,\n#   linear_12 &lt;fct&gt;\n‚Ä¶we‚Äôd like to model the class using the remainder of the variables in this dataset using a logistic regression. We can using the following code to do so:\nfit(logistic_reg(), class ~ ., d)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = class ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n        (Intercept)  two_factor_1level_2         two_factor_2  \n           7.080808            -7.463598            -2.821078  \nnon_linear_1level_2         non_linear_2         non_linear_3  \n           0.550041            -0.878752            -2.039599  \n          linear_01            linear_02            linear_03  \n          -0.153653            -0.273809            -0.005318  \n          linear_04            linear_05            linear_06  \n           1.248566             0.880917            -0.112520  \n          linear_07            linear_08            linear_09  \n          -0.925063             1.261782            -1.251249  \n   linear_10level_2     linear_10level_3     linear_10level_4  \n         -18.020101            -3.242219            -1.336644  \n   linear_11level_2     linear_11level_3  \n          -0.640718            -4.402850  \n [ reached getOption(\"max.print\") -- omitted 4 entries ]\n\nDegrees of Freedom: 99 Total (i.e. Null);  76 Residual\nNull Deviance:      134.6 \nResidual Deviance: 54.75    AIC: 102.8\nThe default engine for a logistic regression in tidymodels is stats::glm(). So, in the style of the above graphic, this code:\nAgain, we can control what happens in steps 1) and 3), but step 2) belongs to the stats package.\nThe time that steps 1) and 3) take is relatively independent of the dimensionality of the training data. That is, regardless of whether we train on one hundred or a million data points, our code (as in, the translation) takes about the same time to run. Regardless of training set size, our code pushes around small, relational data structures to determine how to correctly interface with a given engine. The time it takes to run step 2), though, depends almost completely on the size of the data. Depending on the modeling engine, modeling 10 times as much data could result in step 2) taking twice as long, or 10x as long, or 100x as long as the original fit.\nSo, while the absolute time allotted to steps 1) and 3) is fixed, the portion of total time to fit a model with tidymodels that is ‚Äúoverhead‚Äù depends on how quick the engine code itself is. How quick is a logistic regression with glm() on 100 data points?\nbench::mark(\n  fit = glm(class ~ ., family = binomial, data = d)\n) %&gt;% \n  select(expression, median)\n\n# A tibble: 1 √ó 2\n  expression   median\n* &lt;bch:expr&gt; &lt;bch:tm&gt;\n1 fit          2.33ms\nAbout a millisecond. That means that, if the tidymodels overhead is one second, we‚Äôve made this model fit a thousand times slower!\nIn practice, the overhead here has hovered around a millisecond or two for the last couple years, and machine learning practitioners usually fit much more computationally expensive models than a logistic regression on 100 data points. You‚Äôll just have to believe me on that second point. Regarding the first:\nbm_logistic_reg &lt;- \n  bench::mark(\n    parsnip = fit(logistic_reg(), class ~ ., d),\n    stats = glm(class ~ ., family = binomial, data = d),\n    check = FALSE\n  )\nRemember that the first expression calls the second one, so the increase in time from the second to the first is the ‚Äúoverhead.‚Äù In this case, it‚Äôs 0.7819108 milliseconds, or 25.5% of the total elapsed time.\nSo, to fit a boosted tree model on 1,000,000 data points, step 2) might take a few seconds. Steps 1) and 3) don‚Äôt care about the size of the data, so they still take a few thousandths of a second. No biggie‚Äîthe overhead is negligible. Let‚Äôs quickly back that up by fitting boosted tree models on simulated datasets of varying sizes, once with the XGBoost interface and once with parsnip‚Äôs wrapper around it.\nTODO: write caption\nThis graph shows the gist of tidymodels‚Äô overhead for modeling engines: as dataset size and model complexity grow larger, model fitting and prediction take up increasingly large proportions of the total evaluation time.\nSection 1.1.3 showed a number of ways users can cut down on the evaluation time of their tidymodels code. Making use of parallelism, reducing the total number of model fits needed to search a given grid, and carefully constructing that grid to search over are all major parts of the story",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "02-models.html#tidymodels-overhead",
    "href": "02-models.html#tidymodels-overhead",
    "title": "2¬† Models",
    "section": "",
    "text": "A graphic representing the tidymodels interface. In order, step 1 ‚Äútranslate‚Äù, step 2 ‚Äúcall‚Äù, and step 3 ‚Äútranslate‚Äù, outline the process of translating from the standardized tidymodels interface to an engine‚Äôs specific interface, calling the modeling engine, and translating back to the standardized tidymodels interface. Step 1 and step 3 are in green, while step 2 is in orange.\n\n\n\n\n\n\n\n\n\nTranslates the tidymodels code, which is consistent across engines, to the format that is specific to the chosen engine. In this case, there‚Äôs not a whole lot to do: it passes the preprocessor as formula, the data as data, and picks a family of stats::binomial.\nCalls stats::glm() and collects its output.\nTranslates the output of stats::glm() back into a standardized model fit object.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "02-models.html#benchmarks",
    "href": "02-models.html#benchmarks",
    "title": "2¬† Models",
    "section": "2.2 Benchmarks",
    "text": "2.2 Benchmarks\n\n2.2.1 Linear models\n\n\n2.2.2 Decision trees\n\n\n2.2.3 Boosted trees\nXGBoost and LightGBM ‚Äì comparison timings for the same thing but from the Python interface?\n\n\n2.2.4 Random forests\n\n\n2.2.5 Support vector machines",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html",
    "href": "03-parallelism.html",
    "title": "3¬† Parallel computing",
    "section": "",
    "text": "3.1 Across-models\nIn Chapter 1, one of the changes I made to greatly speed up that resampling process was to introduce an across-models parallel backend. By ‚Äúacross-models,‚Äù I mean that each individual model fit happens on a single CPU core, but I allot each of the CPU cores I‚Äôve reserved for training a number of model fits to take care of.\nPhrased another way, in the case of ‚Äúsequential‚Äù training, all 120 model fits happened one after the other.\nWhile the one CPU core running my R process works itself to the bone, the other remaining 9 are mostly sitting idle (besides keeping my many browser tabs whirring). CPU parallelism is about somehow making use of more cores than the one my main R process is running on; in theory, if \\(n\\) times as many cores are working on fitting models, the whole process could take \\(1/n\\) of the time.\nWhy do I say ‚Äúin theory‚Äù? The orchestration of splitting up that work is actually a very, very difficult problem, for two main reasons:\nThere are two dominant approaches to distributing model fits across local cores that I‚Äôve hinted at already: forking and socket clusters. We‚Äôll delve further into the weeds of each of those approaches in the coming subsections. At a high level, though, the folk knowledge is that forking is subject to less overhead in sending data back and forth, but has some quirks that make it less portable (more plainly, it‚Äôs not available on Windows) and a bit unstable thanks to a less-than-friendly relationship with R‚Äôs garbage collector. As for load balancing, the choice between these two parallelism techniques isn‚Äôt really relevant.\nBefore I spend time experimenting with these techniques, I want to quickly situate the terminology I‚Äôm using here in the greater context of discussions of parallel computing with R. ‚ÄúSequential,‚Äù ‚Äúforking,‚Äù and ‚Äúsocket clusters‚Äù are my preferred terms for the techniques I‚Äôll now write about, but there‚Äôs quite a bit of diversity in the terminology folks use to refer to them. I‚Äôve also called out keywords (as in, functions or packages) related to these techniques in various generations of parallel computing frameworks in R. In ‚Äúbase,‚Äù I refer to functions in the parallel package, building on popular packages multicore (first on CRAN in 2009, inspiring mclapply()) and snow (first on CRAN in 2003, inspiring parLapply()) and included in base installations of R from 2011 onward (R Core Team 2024).",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-across-models",
    "href": "03-parallelism.html#sec-across-models",
    "title": "3¬† Parallel computing",
    "section": "",
    "text": "Figure¬†3.1\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.2\n\n\n\n\n\nGetting data from one place to another: Each of us has likely spent minutes or even hours waiting for a call to load() on a big old .RData object to complete. In that situation, data is being brought in from our hard disk or a remote database (or even elsewhere in memory) into memory allocated by our R process. R is a single-process program, so in order to train multiple models simultaneously, we need multiple R processes, each able to access the training data. We then have two options: one would be to somehow allow each of those R processes to share one copy of the data (this is the idea behind forking, described in Section 3.1.2), the other to send a copy of the data to each process (the idea behind socket clusters, described in Section 3.1.3). The former sounds nice but can become a headache quite quickly. The latter sounds computationally expensive but, with enough memory and sufficiently low latency in copying data (as would be the case with a set of clusters living on one laptop), can often outperform forking for local workflows.\nLoad balancing: Imagine I have some machine learning model with a hyperparameter \\(p\\), and that the computational complexity of that model is such that, with hyperparameter value \\(p\\), the model takes \\(p\\) minutes to train. I am trying out values of \\(p\\) in \\(1, 2, 3, ..., 40\\) and distributing model training across 5 cores. Without the knowledge of how \\(p\\) affects training times, I might send models with \\(p\\) in \\(1, 2, 3, ...8\\) off to the first core, \\(9, 10, 11, ..., 16\\) off to the second core, and so on. In this case, the first core would finish up all of its fits in a little over half an hour while the last would take almost 5 hours. In this example, I‚Äôve taken the penalty on overhead of sending all of the training data off to each core, but in the end, one core ends up doing the majority of the work anyway. In this case, too, we were lucky that the computational complexity of model fits relative to this parameter were roughly linear‚Äîit‚Äôs not uncommon for model fit times to have a quadratic or geometric relationship with the values of important hyperparameters. A critical reader might have two questions. The first: if the computational complexity relative to this parameter is known, why don‚Äôt you just batch the values of \\(p\\) up such that each worker will take approximately the same amount of time? This is a reasonable question, and it relates to the hard problem of chunking. In some situations, related to individual parameters, it really is just about this simple to determine the relationship between parameter values and fit times. In reality, those relationships tend not to be quite so clear-cut, and even when they are, the implications of that parameter value for fit times often depend on the values of other parameters; a pairing of some parameter value \\(p\\) with some other value of a different parameter \\(q\\) might cause instability in some gradient descent process or otherwise, making the problem of estimating the fit time of a model given some set of parameter values a pretty difficult problem for some model types. The second question: couldn‚Äôt you just send each of the cores a single parameter value and have them let the parent R process know they‚Äôre done, at which point they‚Äôll receive another parameter value to get to work on evaluating? That way, the workers that happen to end up with a quicker-fitting values earlier on won‚Äôt sit idle waiting for other cores to finish. This approach is called asynchronous (or ‚Äúasync‚Äù) and, in some situations, can be quite helpful. Remember, though, that this requires getting data (in the form of the communication that a given worker is done evaluating a model, and maybe passing along some performance metric values) back and forth much more often. If the overhead of that communication exceeds the time that synchronous workers had spent idle, waiting for busier cores to finish running, then the asynchronous approach will result in a net slowdown.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nfuture\nforeach\nbase\nSynonyms\n\n\n\n\nSequential\nsequential()\n\n\nSerial\n\n\nForking\nmulticore()\ndoMC\nmclapply()\nProcess forking\n\n\nSocket Clusters\nmultisession()\ndoParallel\nparLapply()\nParallel Socket Clusters (PSOCK), Cluster, Socket\n\n\n\n\n3.1.1 Sequential\nGenerally, in this chapter, I‚Äôm writing about various approaches to parallel computing. I‚Äôll compare each of those approaches to each, but also to the sequential (or ‚Äúnot parallel‚Äù) approach. Sequential evaluation means evaluating model fits in sequence, or one after the other.\nTo demonstrate the impacts of different parallelism approaches throughout this chapter, we‚Äôll always start the conversation with a short experiment. I‚Äôll define a function that tracks the elapsed time to resample a boosted tree ensemble against simulated data, given a number of rows to simulate and a parallelism approach.\n\ntime_resample_bt &lt;- function(n_rows, plan) {\n  # simulate data with n_rows rows\n  set.seed(1)\n  d &lt;- simulate_regression(n_rows)\n  \n  # set up a parallelism plan\n  # set `workers = 4`, which will be ignored for `plan = \"sequential\"`.\n  if (plan == \"multicore\") {\n    rlang::local_options(parallelly.fork.enable = TRUE)\n  }\n  if (plan == \"multisession\") {\n    rlang::local_options(future.globals.maxSize = 1024*1024^2) # 1gb\n  }\n  suppressWarnings(\n    plan(plan, workers = 4) \n  )\n  \n  # track the elapsed time to...\n  bench::mark(\n    resample =\n      fit_resamples(\n        # ...evaluate a boosted tree ensemble...\n        boost_tree(\"regression\"),\n        # ...modeling the outcome using all predictors...\n        outcome ~ .,\n        # ...against a 10-fold cross-validation of `d`.\n        vfold_cv(d, v = 10)\n      ),\n    memory = FALSE\n  )\n}\n\nHere‚Äôs a quick example, simulating 100 rows of data and evaluating its resamples sequentially:\n\nt_seq &lt;- time_resample_bt(100, \"sequential\")\n\nt_seq\n\n\nt_seq\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      201ms    2.19MB\n\n\nIn total, the whole process took 0.2 seconds on my laptop. This expression, among other things, fits a model for each resample on \\(n * \\frac{v-1}{v} = 100 * \\frac{9}{10} = 90\\) rows, meaning that even if the model fits took up 100% of the evaluation time in total, they take 0.02 seconds each. In other words, these fits are quite fast. As such, the overhead of distributing computations across cores would have to be quite minimal in order to see speedups with computations done in parallel. Scaling up the number of rows in the training data, though, results in elapsed times becoming a bit more cumbersome; in the following code, we‚Äôll resample models on datasets with 100 to a million rows, keeping track of the elapsed time for each iteration.\n\npress_seq &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"sequential\"),\n    n_rows = 10^(2:6)\n  )\n\nFor now, the graph we can put together with this data isn‚Äôt super interesting:\n\nggplot(press_seq) +\n  aes(x = n_rows, y = median) +\n  scale_x_log10() +\n  geom_line(color = \"#cd6f3d\") +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption\n\n\n\n\nMore rows means a longer fit time‚Äîwhat a thrill! In the following sections, we‚Äôll compare this timing to those resulting from different parallelism approaches.\n\n\n3.1.2 Forking\nProcess forking is a mechanism where an R process creates an exact copy of itself, called a ‚Äúchild‚Äù process (or ‚Äúworker.‚Äù) Initially, workers share memory with the original (‚Äúparent‚Äù) process, meaning that there‚Äôs no overhead resulting from creating multiple copies of training data to send out to workers. So, in the case of tidymodels, each worker needs to have some modeling packages loaded and some training data available to get started on evaluating a model workflow against resample; those packages and data are already available in the parent process, so tidymodels should see very little overhead in shipping data off to workers with forking.\n\nThere are a few notable drawbacks of process forking:\n\nThere‚Äôs no direct way to ‚Äúfork‚Äù a process from one machine to another, so forking is available only on a single machine. To distribute computations across multiple machines, practitioners will need to make use of socket clusters (described in the following section Section 3.1.3).\nForking is based on the operating system command fork, available only on Unix-alikes (i.e.¬†macOS and Linux). Windows users are out of luck.\nIn practice, memory that is initially shared often ends up ultimately copied due to R‚Äôs garbage collection. ‚Äú[I]f the garbage collector starts running in one of the forked [workers], or the [parent] process, then that originally shared memory can no longer be shared and the operating system starts copying memory blocks into each [worker]. Since the garbage collector runs whenever it wants to, there is no simple way to avoid this‚Äù (Bengtsson 2021).\n\nLet‚Äôs rerun that experiment from the previous section using forking and compare timings.\n\npress_fork &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"multicore\"),\n    n_rows = 10^(2:6)\n  )\n\nWe can make the plot from the last section a bit more interesting now.\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\")\n) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption\n\n\n\n\nWell, if that ain‚Äôt by the book! For the smallest training dataset, n = 100, distributing computations across cores resulted in a new slowdown. Very quickly, though, forking meets up with the sequential approach in elapsed time, and by the time we‚Äôve made it to more realistic dataset sizes, forking almost always wins. In case the log scale is tripping you up, here are the raw timings for the largest dataset:\n\n\n# A tibble: 2 √ó 3\n    median Approach    n_rows\n* &lt;bch:tm&gt; &lt;chr&gt;        &lt;dbl&gt;\n1    4.13m Sequential 1000000\n2    1.34m Forking    1000000\n\n\n\nAnother angle from which to poke at this is how much time did we lose to overhead? Said another way, if I distributed these computations across 4 cores, then I should see a 4-fold speedup in a perfect world. If I divide the time it takes to resample this model sequentially by 4, how does it compare to the timing I observe with forking?\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\")\n) %&gt;%\n  mutate(median_adj = case_when(\n    Approach == \"Sequential\" ~ median / 4,\n    .default = median\n  )) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median_adj, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time Per Core)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption\n\n\n\n\nPer core, no parallel approach will ever be faster than sequential. (If it is, there‚Äôs a performance bug in your code!) As the computations that happen in workers take longer and longer, though, the overhead shrinks as a proportion of the total elapsed time.\n\n\n3.1.3 Socket Clusters\n\nLet‚Äôs see how this plays out in practice. Returning to the same experiment from before:\n\npress_sc &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"multisession\"),\n    n_rows = 10^(2:6)\n  )\n\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\"),\n  mutate(press_sc, Approach = \"Socket Clusters\")\n) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-within-models",
    "href": "03-parallelism.html#sec-within-models",
    "title": "3¬† Parallel computing",
    "section": "3.2 Within-models",
    "text": "3.2 Within-models\nI‚Äôve only written so far about across-model parallelism, where multiple CPU cores are used to train a set of models but individual model fits happen on a single core. For most machine learning models available via tidymodels, this is the only type of parallelism possible. However, some of the most well-used modeling engines in tidymodels, like XGBoost and LightGBM, allow for distributing the computations to train a single model across several CPU cores (or even on GPUs). This begs the question, then, of whether tidymodels users should always stick to across-model parallelism, to within-model parallelism for the models it‚Äôs available for, or to a hybrid of both. We‚Äôll focus first on the former two options in this subsection and then explore their interactions in Section 3.3.\n\n3.2.1 CPU\n\n\n3.2.2 GPU\nXGBoost parallelizes at a finer level (individual trees and split finding), while LightGBM parallelizes at a coarser level (features and data subsets).\nXGBoost\n\narg device is cpu or cuda (or gpu, but cuda is the only supported device).\narg nthread is integer\nuses openMP for cpu (?)\ngpu_hist is apparently pretty ripping?\n\nLightGBM\n\ndevice_type is cpu, gpu, or cuda (fastest, but requires GPUs supporting CUDA)\ncuda is fastest but only available on Linux with NVIDIA GPUs with compute capability 6.0+\ngpu is based on OpenCL‚Ä¶ M1 Pro is a ‚Äúbuilt-in‚Äù / ‚Äúintegrated‚Äù\ncpu uses OpenMP for CPU parallelism\nset to real CPU cores (i.e.¬†not threads)\nrefer to Installation Guide to build LightGBM with GPU or CUDA support\nare num_threads is integer\nDask available to Python users\n\naorsf\n\nn_thread implements OpenMP CPU threading\n\nkeras (MLP)\n\nCPU or GPU\ncuda (available only for jax backend?)\nalso available with tensorflow\n\nh2o stuff\nbaguette\n\ncontrol_bag(allow_parallel)? does this respect nested parallelism?\n\nWhile GPU support is available for both libraries in R, it‚Äôs not as straightforward to use as in Python. The R ecosystem generally has less robust GPU support compared to Python, which can make GPU-accelerated machine learning in R more challenging to set up and use.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-within-and-across",
    "href": "03-parallelism.html#sec-within-and-across",
    "title": "3¬† Parallel computing",
    "section": "3.3 Within and Across",
    "text": "3.3 Within and Across",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#strategy",
    "href": "03-parallelism.html#strategy",
    "title": "3¬† Parallel computing",
    "section": "3.4 Strategy",
    "text": "3.4 Strategy\nChoosing n cores and parallel_over‚Ä¶",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "03-parallelism.html#distributed-computing",
    "href": "03-parallelism.html#distributed-computing",
    "title": "3¬† Parallel computing",
    "section": "3.5 Distributed Computing",
    "text": "3.5 Distributed Computing\nSo far in this chapter, I‚Äôve mostly focused on the performance considerations for distributing computations across cores on a single computer. Distributed computing ‚Äúin the cloud,‚Äù where data is shipped off for processing on several different computers, is a different ball-game. That conversation is mostly outside of the scope of this book, but I do want to give some high-level intuition on how local parallelism differs from distributed parallelism.\nThe idea that will get you the most mileage in reasoning about distributed parallelism is this: the overhead of sending data back and forth is much more substantial in distributed computing than it is in the local context.\nIn order for me to benefit from distributing these computations across cores, the overhead of sending data out to workers has to be so minimal that it doesn‚Äôt overtake the time saved in distributing model fits across cores. Let‚Äôs see how this plays out on my laptop, first:\n\nt_par &lt;- time_resample_bt(100, \"multisession\")\n\nt_par\n\n\nt_par\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      242ms      15MB\n\n\nNo dice! As we know from earlier on, though, we‚Äôll start to see a payoff when model fits take long enough to outweigh the overhead of sending data back and forth between workers. But, but! Remember the high-mileage lesson: this overhead is greater for distributed systems. Let‚Äôs demonstrate this.\nI‚Äôll first run this experiment for numbers of rows \\(100, 1000, ..., 1,000,000\\) both sequentially and via socket clusters on my laptop, recording the timings as I do so. Then, I‚Äôll do the same thing on a popular data science hosted service, and we‚Äôll compare results.\n\nbench::press(\n  time_resample_bt(n_rows, plan),\n  n_rows = 10^(2:6),\n  plan = c(\"sequential\", \"multisession\")\n)\n\nThe resulting timings are in the object timings and look like this:\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  head()\n\n# A tibble: 6 √ó 4\n   n_rows plan           median system\n*   &lt;dbl&gt; &lt;chr&gt;        &lt;bch:tm&gt; &lt;chr&gt; \n1     100 sequential   216.08ms laptop\n2    1000 sequential   442.77ms laptop\n3   10000 sequential      2.81s laptop\n4  100000 sequential      27.8s laptop\n5 1000000 sequential      4.03m laptop\n6     100 multisession 247.13ms laptop\n\n\nThere‚Äôs one timing per unique combination of number of rows, parallelism plan, and system (\"laptop\" vs \"cloud\"). Based on these timings, we can calculate the factor of speedup for sequential evaluation versus its parallel analogue.\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  pivot_wider(names_from = plan, values_from = median) %&gt;%\n  mutate(\n    speedup = as.numeric(sequential / multisession),\n    speedup = if_else(speedup &lt; 1, -1/speedup, speedup)\n  ) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = speedup, col = system) +\n  geom_point() +\n  scale_x_log10() +\n  labs(\n    x = \"Log(Number of Rows)\", \n    y = \"Signed Factor of Speedup\", \n    col = \"System\"\n  )\n\n\n\n\nTODO: write caption\n\n\n\n\nIn this plot, a signed speedup value of 2 would mean that the socket cluster (i.e., parallel) approach was twice as fast, while a value of -2 would mean that the sequential approach ran twice as fast as the socket cluster approach. In general, larger numbers of rows (and thus longer-running model fits) tend to be associated with greater speedups as a result of switching to parallel computing. For local clusters on my laptop, the overhead of passing data around is small enough that I start to see a payoff when switching to parallel computing for only 1000 rows. As for the cloud system, though, model fits have to take a long time before switching to parallel computing begins to reduce elapsed times.\nThe conclusion to draw here is not that one ought not to use hosted setups for parallel computing. Aside from the other many benefits of doing data analysis in hosted environments, some of these environments enable ‚Äúmassively parallel‚Äù computing, or, in other words, a ton of cores. In the example shown in this chapter, we fitted a relatively small number of models, so we wouldn‚Äôt necessarily benefit (and would likely see a slowdown) from scaling up the number of cores utilized. If we had instead wanted to tune that boosted tree over 1,000 proposed hyperparameter combinations‚Äîrequiring 10,000 model fits with a 10-fold resampling scheme‚Äîwe would likely be much better off utilizing a distributed system with higher latency (and maybe even less performant individual cores) and 1,000 or 10,000 cores.\n\n\n\n\nBengtsson, Henrik. 2021. ‚ÄúA Unifying Framework for Parallel and Distributed Processing in R using Futures.‚Äù The R Journal 13 (2): 273‚Äì91. https://doi.org/10.32614/RJ-2021-048.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "04-search.html",
    "href": "04-search.html",
    "title": "4¬† Search",
    "section": "",
    "text": "4.1 Grid search",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "04-search.html#iterative-search",
    "href": "04-search.html#iterative-search",
    "title": "4¬† Search",
    "section": "4.2 Iterative Search",
    "text": "4.2 Iterative Search\n\n4.2.1 Simulated Annealing\n\n\n4.2.2 Bayesian Optimization",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "05-submodel.html",
    "href": "05-submodel.html",
    "title": "5¬† The submodel trick",
    "section": "",
    "text": "5.1 Demonstration\nRecall that, in Section 1.1.2, we resampled an XGBoost boosted tree model to predict whether patients would be readmitted within 30 days after an inpatient hospital stay. Using default settings with no optimizations, the model took 3.68 hours to resample. With a switch in computation engine, implementation of parallel processing, change in search strategy, and enablement of the submodel trick, the time to resample was knocked down to 1.52 minutes. What is the submodel trick, though, and what was its individual contribution to that speedup?\nFirst, loading packages and setting up resamples and the model specification as before:\n# load packages\nlibrary(tidymodels)\nlibrary(readmission)\n\n# load and split data:\nset.seed(1)\nreadmission_split &lt;- initial_split(readmission)\nreadmission_train &lt;- training(readmission_split)\nreadmission_test &lt;- testing(readmission_split)\nreadmission_folds &lt;- vfold_cv(readmission_train)\n\n# set up model specification\nbt &lt;- \n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "05-submodel.html#demonstration",
    "href": "05-submodel.html#demonstration",
    "title": "5¬† The submodel trick",
    "section": "",
    "text": "5.1.1 Grids\nIf I just pass these resamples and model specification to tune_grid(), as I did in Section 1.1.2, tidymodels will take care of generating the grid of parameters to evaluate itself. By default, tidymodels generates grids of parameters using an experimental design called a latin hypercube (McKay, Beckman, and Conover 2000; Dupuy, Helbert, and Franco 2015). We can use the function grid_latin_hypercube() from the dials package to replicate the same grid that tune_grid() had generated under the hood:\n\nset.seed(1)\nbt_grid_latin_hypercube &lt;- \n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_latin_hypercube(size = 12)\n\nbt_grid_latin_hypercube\n\n# A tibble: 12 √ó 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1   647    0.164  \n 2   404    0.0555 \n 3  1798    0.0452 \n 4   796    0.274  \n 5  1884    0.00535\n 6  1139    0.00137\n 7  1462    0.105  \n 8  1201    0.00194\n 9   867    0.0225 \n10   166    0.00408\n11   326    0.0116 \n12  1608    0.00997\n\n\nSince we‚Äôre working with a two-dimensional grid in this case, we can plot the resulting grid to get a sense for the distribution of values:\n\nggplot(bt_grid_latin_hypercube) +\n  aes(x = trees, y = learn_rate) +\n  geom_point()\n\n\n\n\nTODO: write caption\n\n\n\n\nWhile the details of sampling using latin hypercubes are not important to understand this chapter, note that values are not repeated in a given dimension. Said another way, we get 12 unique values for trees, and 12 unique values for learn_rate. Juxtapose this with the design resulting from grid_regular() used in Section 1.1.3:\n\nset.seed(1)\nbt_grid_regular &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\nbt_grid_regular\n\n# A tibble: 16 √ó 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1     1    0.001  \n 2   667    0.001  \n 3  1333    0.001  \n 4  2000    0.001  \n 5     1    0.00681\n 6   667    0.00681\n 7  1333    0.00681\n 8  2000    0.00681\n 9     1    0.0464 \n10   667    0.0464 \n11  1333    0.0464 \n12  2000    0.0464 \n13     1    0.316  \n14   667    0.316  \n15  1333    0.316  \n16  2000    0.316  \n\n\nPlotting in the same way:\n\nggplot(bt_grid_regular) +\n  aes(x = trees, y = learn_rate) +\n  geom_point()\n\n\n\n\nTODO: write caption\n\n\n\n\nThe argument levels = 4 indicates that \\(4\\) values are generated individually for each parameter, and the resulting grid is created by pairing up each unique combination of those values.\nYou may have noticed that this regular grid contains even more proposed points‚Äî4 x 4 = 16‚Äîthan the latin hypercube with size = 12. A reasonable question, then: how on earth would the larger grid be resampled more quickly than the smaller one? It‚Äôs possible that I hid some slowdown resulting from this larger grid among the rest of the optimizations implemented in Section 1.1.3; lets test the effect of the change in grid by itself.\n\nset.seed(1)\n\nbm_grid_regular &lt;- \n  bench::mark(\n    grid_regular = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid_regular\n      )\n  )\n\n\nbm_grid_regular\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 basic         1.23h    3.91GB\n\n\nSee? Have some faith in me!\nChanging only the grid argument (and even increasing the number of proposed grid points), we‚Äôve decreased the time to evaluate against resamples from 3.68 to 1.23 hours, or a speedup of -99.67%.\n\n\n5.1.2 The trick\nPassing this regular grid allowed tune_grid() to use what the tidymodels team refers to as the ‚Äúsubmodel trick,‚Äù where many more models can be evaluated than were actually fit.\nTo best understand how the submodel trick works, let‚Äôs refresh on how boosted trees work. The training process begins with a simple decision tree, and subsequent trees are added iteratively, each one correcting the errors of the previous trees by focusing more on the data points associated with the greatest error. The final model is a weighted sum of all the individual trees, where each tree contributes to reducing the overall error.\nSo, for example, to train a boosted tree model with 2000 trees, we first need to train a boosted tree model with 1 tree. Then, we need to take that model, figure out where it made its largest errors, and train a second tree that aims to correct those errors. So on, until the, say, 667-th tree, and so on until the 1333-rd tree, and so on until, finally, the 2000-th tree. Picking up what I‚Äôm putting down? Along the way to training a boosted tree with 2000 trees, we happened to train a bunch of other models we might be interested in evaluating: what we call submodels. So, in the example of bt_grid_regular, for a given learn_rate, we only need to train the model with the maximum trees. In this example, that‚Äôs a quarter of the model fits.\n\n\n\n\n\n\nNote\n\n\n\nYou might note that we don‚Äôt see a speedup nearly as drastic as 4 times. While we indeed only fit a quarter of the models, we‚Äôre fitting the boosted trees with the largest number of trees, and the time to train a boosted tree scales linearly with the number of trees. Said another way, we‚Äôre eliminating the need to fit only the faster-fitting models. This tends to be the case in many cases where the submodel trick applies.\n\n\nTo evaluate a fitted model with performance metrics, all we need are its predictions (and, usually, the true values being predicted). In pseudocode, resampling a model against performance metrics usually goes something like this:\n\nfor (resample in resamples) {\n  # analogue to the \"training\" set for the resample\n  analysis &lt;- analysis(resample)\n  # analogue to the \"testing\" set for the resample\n  assessment &lt;- assessment(resample)\n  \n  for (model in models) {\n    # the longest-running operation:\n    model_fit &lt;- fit(model, analysis)\n    \n    # usually, comparatively quick operations:\n    model_predictions &lt;- predict(model_fit, assessment)\n    metrics &lt;- c(metrics, metric(model_predictions))\n  }\n}\n\n\n\n\n\n\n\nNote\n\n\n\nanalysis(), assessment(), fit(), and predict() are indeed actual functions in tidymodels. metric() is not, but an analogue could be created from the output of the function metric_set().\n\n\nAmong all of these operations, fitting the model with fit() is usually the longest-running step, by far. In comparison, predict()ing on new values and calculating metrics takes very little time. Using the submodel trick allows us to reduce the number of fit()s while keeping the number of calls to predict() and metric() constant. In pseudocode, resampling with the submodel trick could look like:\n\nfor (resample in resamples) {\n  analysis &lt;- analysis(resample)\n  assessment &lt;- assessment(resample)\n  \n  models_to_fit &lt;- models[unique(non_submodel_args)]\n  \n  for (model in models_to_fit) {\n    model_fit &lt;- fit(model, analysis)\n    \n    for (model_to_eval in models[unique(submodel_args)]) {\n      model_to_eval &lt;- predict(model_to_eval, assessment)\n      metrics &lt;- c(metrics, metric(model_predictions))\n    }\n  }\n}\n\nThe above pseudocode admittedly requires some generosity (or mental gymnastics) to interpret, but the idea is that if fit()ting is indeed the majority of the time spent in resampling, and models_to_fit contains many fewer elements than models in the preceding pseudocode blocks, we should see substantial speedups.\n\n\n5.1.3 At its most extreme\nIn the applied example above, we saw a relatively modest speedup. If we want to really show off the power of the submodel trick, in terms of time spent resampling per model evaluated, we can come up with a somewhat silly grid:\n\nbt_grid_regular_go_brrr &lt;-\n  bt_grid_regular %&gt;%\n  slice_max(trees, by = learn_rate) %&gt;%\n  map(.x = c(1, seq(10, max(.$trees), 10)), .f = ~mutate(.y, trees = .x), dfr = .) %&gt;%\n  bind_rows()\n\nbt_grid_regular_go_brrr\n\n# A tibble: 804 √ó 2\n   trees learn_rate\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1    0.001  \n 2     1    0.00681\n 3     1    0.0464 \n 4     1    0.316  \n 5    10    0.001  \n 6    10    0.00681\n 7    10    0.0464 \n 8    10    0.316  \n 9    20    0.001  \n10    20    0.00681\n# ‚Ñπ 794 more rows\n\n\nIn this grid, we have the same number of unique values of learn_rate, \\(4\\), resulting in the same \\(4\\) model fits. Except that, in this case, we‚Äôre evaluating every model with number of trees \\(1, 10, 20, 30, 40, ..., 2000\\). If our hypothesis that predicting on the assessment set and generating performance metrics is comparatively fast is true, then we‚Äôll see that elapsed time per grid point is way lower:\n\nset.seed(1)\n\nbm_grid_regular_go_brrr &lt;- \n  bench::mark(\n    grid_regular_go_brrr = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid_regular_go_brrr\n      )\n  )\n\nThe above code took 1.44 hours to run, a bit longer than the 1.23 hours from bm_grid_regular (that fitted the same number of models), but comparable. Per grid point, that difference is huge:\n\n# time per grid point for bm_grid_regular\nbm_grid_regular$median[[1]] / nrow(bt_grid_regular)\n\n[1] 4.62m\n\n# time per grid point for bm_grid_regular_go_brrr\nbm_grid_regular_go_brrr$median[[1]] / nrow(bt_grid_regular_go_brrr)\n\n[1] 6.44s\n\n\nWhether this difference in timing is of practical significance to a user is debatable. In the context where we generated bm_grid_regular, where the grid of points searched over is relatively comparable (and thus similarly likely to identify a performant model) yet the decreased number of model fits gave rise to a reasonable speedup‚Äî-99.67%‚Äîis undoubtedly impactful for many typical use cases of tidymodels. The more eye-popping per-grid-point speedups, as with bm_grid_regular_go_brrr, are more so a fun trick than a practical tool for most use cases.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "05-submodel.html#overview",
    "href": "05-submodel.html#overview",
    "title": "5¬† The submodel trick",
    "section": "5.2 Overview",
    "text": "5.2 Overview\nI‚Äôve demonstrated the impact of the submodel trick in the above section through one example context, tuning trees in an XGBoost boosted tree model. The submodel trick can be found in many places across the tidymodels framework, though.\nSubmodels are defined with respect to a given model parameter, e.g.¬†trees in boost_tree(). While a given parameter often defines a submodel regardless of modeling engine for a given model type‚Äîe.g.¬†trees defines submodels for boost_tree() models regardless of whether the model is fitted with engine = \"xgboost\" or engine = \"lightgbm\"‚Äîthere are some exceptions. Many tidymodels users undoubtedly tune models using arguments that define submodels without even knowing it. Some common examples include:\n\npenalty in linear_reg(), logistic_reg(), and multinom_reg(), which controls the amount of regularization in linear models.\nneighbors in nearest_neighbor(), the number of training data points nearest the point to be predicted that are factored into the prediction.\n\nAgain, some modeling engines for each of these model types do not actually support prediction from submodels from the noted parameter. See Section 5.2.1 for a complete table of currently supported arguments defining submodels in tidymodels.\nIn the above example, we had to manually specify a grid like bt_regular_grid in order for tidymodels to use a grid that can take advantage of the submodel trick. This reflects the frameworks‚Äô general prioritization of predictive performance over computational performance in its design and defaults; latin hypercubes have better statistical properties when it comes to discovering performant hyperparameter combinations than a regular grid (McKay, Beckman, and Conover 2000; Stein 1987; Santner et al. 2003). However, note that in the case where a model is being tuned over only one argument, the submodel trick will kick in regardless of the sampling approach being used: regardless of how a set of univariate points are distributed, the most extreme parameter value (e.g.¬†the max trees) can be used to generate predictions for values across the distribution.\n\n5.2.1 Supported parameters\nA number of tuning parameters support the submodel trick:\n\n\n\n\n\nModel Type\nArgument\nEngines\n\n\n\n\nboost_tree\ntrees\nxgboost, C5.0, lightgbm\n\n\nC5_rules\ntrees\nC5.0\n\n\ncubist_rules\nneighbors\nCubist\n\n\ndiscrim_flexible\nnum_terms\nearth\n\n\nlinear_reg\npenalty\nglmnet\n\n\nlogistic_reg\npenalty\nglmnet\n\n\nmars\nnum_terms\nearth\n\n\nmultinom_reg\npenalty\nglmnet\n\n\nnearest_neighbor\nneighbors\nkknn\n\n\npls\nnum_comp\nmixOmics\n\n\npoisson_reg\npenalty\nglmnet\n\n\nproportional_hazards\npenalty\nglmnet\n\n\nrule_fit\npenalty\nxrf\n\n\n\n\n\n\n\n\n\nDupuy, Delphine, C√©line Helbert, and Jessica Franco. 2015. ‚ÄúDiceDesign and DiceEval: Two r Packages for Design and Analysis of Computer Experiments.‚Äù Journal of Statistical Software 65 (11).\n\n\nMcKay, Michael D, Richard J Beckman, and William J Conover. 2000. ‚ÄúA Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.‚Äù Technometrics 42 (1): 55‚Äì61.\n\n\nSantner, Thomas J, Brian J Williams, William I Notz, and Brain J Williams. 2003. The Design and Analysis of Computer Experiments. Vol. 1. Springer.\n\n\nStein, Michael. 1987. ‚ÄúLarge Sample Properties of Simulations Using Latin Hypercube Sampling.‚Äù Technometrics 29 (2): 143‚Äì51.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "06-extras.html",
    "href": "06-extras.html",
    "title": "6¬† Extras",
    "section": "",
    "text": "6.1 Preprocessing",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "06-extras.html#sec-sparsity",
    "href": "06-extras.html#sec-sparsity",
    "title": "6¬† Extras",
    "section": "6.2 Sparsity",
    "text": "6.2 Sparsity",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "06-extras.html#sec-gpu-training",
    "href": "06-extras.html#sec-gpu-training",
    "title": "6¬† Extras",
    "section": "6.3 GPU Training",
    "text": "6.3 GPU Training",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "06-extras.html#sec-stacking",
    "href": "06-extras.html#sec-stacking",
    "title": "6¬† Extras",
    "section": "6.4 Stacking",
    "text": "6.4 Stacking",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bengtsson, Henrik. 2021. ‚ÄúA Unifying\nFramework for Parallel and Distributed Processing in R using\nFutures.‚Äù The R Journal 13 (2):\n273‚Äì91. https://doi.org/10.32614/RJ-2021-048.\n\n\nDupuy, Delphine, C√©line Helbert, and Jessica Franco. 2015.\n‚ÄúDiceDesign and DiceEval: Two r Packages for Design and Analysis\nof Computer Experiments.‚Äù Journal of Statistical\nSoftware 65 (11).\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with\nR. \" O‚ÄôReilly Media, Inc.\".\n\n\nMcKay, Michael D, Richard J Beckman, and William J Conover. 2000.\n‚ÄúA Comparison of Three Methods for Selecting Values of Input\nVariables in the Analysis of Output from a Computer Code.‚Äù\nTechnometrics 42 (1): 55‚Äì61.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nSantner, Thomas J, Brian J Williams, William I Notz, and Brain J\nWilliams. 2003. The Design and Analysis of Computer\nExperiments. Vol. 1. Springer.\n\n\nStein, Michael. 1987. ‚ÄúLarge Sample Properties of Simulations\nUsing Latin Hypercube Sampling.‚Äù Technometrics 29 (2):\n143‚Äì51.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O‚ÄôReilly Media, Inc.\".",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "03-parallelism.html#sec-distributed-computing",
    "href": "03-parallelism.html#sec-distributed-computing",
    "title": "3¬† Parallel computing",
    "section": "3.5 Distributed Computing",
    "text": "3.5 Distributed Computing\nSo far in this chapter, I‚Äôve mostly focused on the performance considerations for distributing computations across cores on a single computer. Distributed computing ‚Äúin the cloud,‚Äù where data is shipped off for processing on several different computers, is a different ball-game. That conversation is mostly outside of the scope of this book, but I do want to give some high-level intuition on how local parallelism differs from distributed parallelism.\nThe idea that will get you the most mileage in reasoning about distributed parallelism is this: the overhead of sending data back and forth is much more substantial in distributed computing than it is in the local context.\nIn order for me to benefit from distributing these computations across cores, the overhead of sending data out to workers has to be so minimal that it doesn‚Äôt overtake the time saved in distributing model fits across cores. Let‚Äôs see how this plays out on my laptop, first:\n\nt_par &lt;- time_resample_bt(100, \"multisession\")\n\nt_par\n\n\nt_par\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      242ms      15MB\n\n\nNo dice! As we know from earlier on, though, we‚Äôll start to see a payoff when model fits take long enough to outweigh the overhead of sending data back and forth between workers. But, but! Remember the high-mileage lesson: this overhead is greater for distributed systems. Let‚Äôs demonstrate this.\nI‚Äôll first run this experiment for numbers of rows \\(100, 1000, ..., 1,000,000\\) both sequentially and via socket clusters on my laptop, recording the timings as I do so. Then, I‚Äôll do the same thing on a popular data science hosted service, and we‚Äôll compare results.\n\nbench::press(\n  time_resample_bt(n_rows, plan),\n  n_rows = 10^(2:6),\n  plan = c(\"sequential\", \"multisession\")\n)\n\nThe resulting timings are in the object timings and look like this:\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  head()\n\n# A tibble: 6 √ó 4\n   n_rows plan           median system\n*   &lt;dbl&gt; &lt;chr&gt;        &lt;bch:tm&gt; &lt;chr&gt; \n1     100 sequential   216.08ms laptop\n2    1000 sequential   442.77ms laptop\n3   10000 sequential      2.81s laptop\n4  100000 sequential      27.8s laptop\n5 1000000 sequential      4.03m laptop\n6     100 multisession 247.13ms laptop\n\n\nThere‚Äôs one timing per unique combination of number of rows, parallelism plan, and system (\"laptop\" vs \"cloud\"). Based on these timings, we can calculate the factor of speedup for sequential evaluation versus its parallel analogue.\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  pivot_wider(names_from = plan, values_from = median) %&gt;%\n  mutate(\n    speedup = as.numeric(sequential / multisession),\n    speedup = if_else(speedup &lt; 1, -1/speedup, speedup)\n  ) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = speedup, col = system) +\n  geom_point() +\n  scale_x_log10() +\n  labs(\n    x = \"Log(Number of Rows)\", \n    y = \"Signed Factor of Speedup\", \n    col = \"System\"\n  )\n\n\n\n\nTODO: write caption\n\n\n\n\nIn this plot, a signed speedup value of 2 would mean that the socket cluster (i.e., parallel) approach was twice as fast, while a value of -2 would mean that the sequential approach ran twice as fast as the socket cluster approach. In general, larger numbers of rows (and thus longer-running model fits) tend to be associated with greater speedups as a result of switching to parallel computing. For local clusters on my laptop, the overhead of passing data around is small enough that I start to see a payoff when switching to parallel computing for only 1000 rows. As for the cloud system, though, model fits have to take a long time before switching to parallel computing begins to reduce elapsed times.\nThe conclusion to draw here is not that one ought not to use hosted setups for parallel computing. Aside from the other many benefits of doing data analysis in hosted environments, some of these environments enable ‚Äúmassively parallel‚Äù computing, or, in other words, a ton of cores. In the example shown in this chapter, we fitted a relatively small number of models, so we wouldn‚Äôt necessarily benefit (and would likely see a slowdown) from scaling up the number of cores utilized. If we had instead wanted to tune that boosted tree over 1,000 proposed hyperparameter combinations‚Äîrequiring 10,000 model fits with a 10-fold resampling scheme‚Äîwe would likely be much better off utilizing a distributed system with higher latency (and maybe even less performant individual cores) and 1,000 or 10,000 cores.\n\n\n\n\nBengtsson, Henrik. 2021. ‚ÄúA Unifying Framework for Parallel and Distributed Processing in R using Futures.‚Äù The R Journal 13 (2): 273‚Äì91. https://doi.org/10.32614/RJ-2021-048.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1 Early value\nTo demonstrate the impact that a couple small pieces of intuition can have on execution time when evaluating machine learning models, I‚Äôll run through a quick model tuning example. On the first go, I‚Äôll lean on tidymodels‚Äô default values and a simple grid search, and on the second, I‚Äôll pull a few tricks out from my sleeves that will drastically reduce the time to evaluate models while only negligibly decreasing predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#early-value",
    "href": "intro.html#early-value",
    "title": "1¬† Introduction",
    "section": "",
    "text": "1.1.1 Setup\nFirst, loading a few needed packages:\n\nlibrary(tidymodels)\nlibrary(future)\nlibrary(finetune)\nlibrary(bonsai)\n\nFor the purposes of this example, we‚Äôll simulate a data set of 100,000 rows and 18 columns. The first column, class, is a binary outcome, and the remaining variables are a mix of numerics and factors.\n\nset.seed(1)\nd &lt;- simulate_classification(1e5)\n\nWe‚Äôll first split the data into training and testing sets before generating a set of 10 folds from the training data for cross-validation.\n\nset.seed(1)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\nd_folds &lt;- vfold_cv(d_train)\n\n\n\n1.1.2 A first go\nFor my first go at tuning, I‚Äôll tune a boosted tree model using grid search. By default, tidymodels will use XGBoost as the modeling engine; I‚Äôll try out a few different values for learn_rate‚Äî a parameter that controls how drastically newly added trees impact predictions‚Äîand trees‚Äîthe number of trees in the ensemble.\n\nbt &lt;- \n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")\n\nI‚Äôll carry out a grid search using tune_grid(), trying out a bunch of different pairs of values for learn_rate and trees and seeing what sticks. The argument grid = 12 indicates that I want to try out 12 different combinations of values and will let tidymodels take care of exactly what those values are.\n\nset.seed(1)\n\nbm_basic &lt;- \n  bench::mark(\n    basic = \n      tune_grid(\n        object = bt,\n        preprocessor = class ~ .,\n        resamples = d_folds,\n        grid = 12\n      )\n  )\n\nbench::mark() returns, among other things, a precise timing of how long this process takes.\n\nbm_basic\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 basic         3.68h    5.43GB\n\n\n\nHoly smokes! 3.68 hours is a good while. What all did tune_grid() do, though?\nFirst, let‚Äôs break down how many model fits actually happened. Since I‚Äôve supplied grid = 12, we‚Äôre evaluating 12 possible model configurations. Each of those model configurations is evaluated against d_folds, a 10-fold cross validation object, meaning that each configuration is fitted 10 times. That‚Äôs 120 model fits!\nFurther, consider that those fits happen on 9/10ths of the training data, or 67500 rows.\n\nWith a couple small changes, though, the time to tune this model can be drastically decreased.\n\n\n1.1.3 A speedy go\nTo cut down on the time to evaluate these models, I‚Äôll make a few small modifications.\nFirst, I‚Äôll evaluate in parallel: Almost all modern laptops have more than one CPU core, and distributing computations across them only takes a couple lines of code with tidymodels.\n\nplan(multisession, workers = 4)\n\nWhile this tuning process could benefit from distributing across many more cores than 4, I‚Äôll just use 4 here to give a realistic picture of the kinds of speedups possible on a typical laptop.\nThen, we‚Äôll use a clever grid: The tidymodels framework enables something called the ‚Äúsubmodel trick,‚Äù a technique that will allow us to predict from many more models than we actually fit. Instead of just supplying grid = 12, I‚Äôll construct the grid myself.\n\nset.seed(1)\nbt_grid &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\n\n\n\n\n\n\nNote\n\n\n\nTo learn more about the submodel trick, see Chapter 5.\n\n\nNext, I‚Äôll switch out the computational engine: Substituting XGBoost with another gradient-boosting model that can better handle some properties of this dataset will cut down on our fit time by a good bit.\n\nbt_lgb &lt;- bt %&gt;% set_engine(\"lightgbm\")\n\nFinally, I‚Äôll give up early on poorly-performing models: Rather than using grid search with tune_grid(), I‚Äôll use a technique called racing that stops evaluating models when they seem to be performing poorly using the tune_race_anova() function.\n\nset.seed(1)\n\nbm_speedy &lt;- \n  bench::mark(\n    speedy = \n      tune_race_anova(\n        object = bt_lgb,\n        preprocessor = class ~ .,\n        resamples = d_folds,\n        grid = bt_grid\n      )\n  )\n\nChecking out the new benchmarks:\n\nbm_speedy\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 speedy        1.52m    47.5MB\n\n\nThe total time to tune was reduced from 3.68 hours to 1.52 minutes‚Äîthe second approach was 145 times faster than the first.\nThe first thing I‚Äôd wonder when seeing this result is how much of a penalty in predictive performance I‚Äôd suffer due to this transition. Let‚Äôs evaluate both of the top models from these tuning results on the test set. First, for the basic workflow:\n\nfit_basic &lt;- \n  select_best(bm_basic$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(class ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = d_split)\n\n\ncollect_metrics(fit_basic)\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.835 Preprocessor1_Model1\n2 roc_auc     binary         0.896 Preprocessor1_Model1\n3 brier_class binary         0.119 Preprocessor1_Model1\n\n\nAs for the quicker approach:\n\nfit_speedy &lt;- \n  select_best(bm_speedy$result[[1]], metric = \"roc_auc\") %&gt;%\n  finalize_workflow(workflow(class ~ ., bt), parameters = .) %&gt;%\n  last_fit(split = d_split)\n\n\ncollect_metrics(fit_speedy)\n\n# A tibble: 3 √ó 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.835 Preprocessor1_Model1\n2 roc_auc     binary         0.895 Preprocessor1_Model1\n3 brier_class binary         0.120 Preprocessor1_Model1\n\n\nVirtually indistinguishable performance results in 0.7% of the time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#our-approach",
    "href": "intro.html#our-approach",
    "title": "1¬† Introduction",
    "section": "1.2 Our approach",
    "text": "1.2 Our approach\nThis book is intended for tidymodels users who have been waiting too long for their code to run. I generally assume that users are familiar with data manipulation and visualization with the tidyverse as well as the basics of machine learning with tidymodels, like evaluating models against resamples using performance metrics. For the former, I recommend (Wickham, √áetinkaya-Rundel, and Grolemund 2023) for getting up to speed‚Äîfor the latter, (Kuhn and Silge 2022). If you‚Äôre generally comfortable with the content in those books, you‚Äôre ready to go.\nModern laptops are remarkable. Users of tidymodels working on many machines made in the last few years are well-prepared to interactively develop machine learning models based on tens of millions of rows of data. That said, without the right information, it‚Äôs quite easy to mistakenly introduce performance issues that result in analyses on even tens of thousands of rows of data becoming too cumbersome to work with. Generally, the tidymodels framework attempts to guard users from making such mistakes and addressing them ourselves when they‚Äôre in our control. At the same time, many foundational and well-used approaches in classical machine learning have well-theorized adaptations that substantially cut down on the elapsed time while preserving predictive performance. The tidymodels framework implements many such adaptations and this book aims to surface them in a holistic and coherent way. Readers will come out of having read this book with a grab bag of one-liners that can cut down on elapsed time to develop machine learning models by orders of magnitude.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#the-hard-part",
    "href": "intro.html#the-hard-part",
    "title": "1¬† Introduction",
    "section": "1.3 The hard part",
    "text": "1.3 The hard part\nTo better understand how to cut down on the time to evaluate models with tidymodels, we need to understand a bit about how tidymodels works.\nLike many other ‚Äúunifying frameworks‚Äù for ML (mlr3, caret, scikit-learn(?)), the tidymodels framework itself does not implement the algorithms to train and predict from models. Instead, tidymodels provides a common interface to modeling engines: packages (or functions from packages) that provide the methods to fit() and predict().\n\n\n\n\n\n\nFigure¬†1.1\n\n\n\nThe process of ‚Äútranslating‚Äù between the tidymodels and engine formats is illustrated in Figure¬†1.1. When fitting and predicting with tidymodels, some portion of the elapsed time to run code is due to the ‚Äútranslation‚Äù of the inputted unified code to the specific syntax that the engine expects, and some portion of it is due to the translation of what the engine returns to the unified output returned by tidymodels; these portions are in the tidymodels team‚Äôs control. The rest of the elapsed time occurs inside of the modeling engine‚Äôs code.\nThe portions of the elapsed time that are in the tidymodels team‚Äôs control are shown in green, and I‚Äôll refer to them in this book as ‚Äúoverhead.‚Äù The overhead of tidymodels in terms of elapsed time is relatively constant with respect to the size of training data. This overhead consists of tasks like checking data types, handling errors and warnings, and‚Äîmost importantly‚Äîprogrammatically assembling calls to engine functions.\nThe portion of the elapsed time shown in orange represents the actual training of (or predicting from) the model. This portion is implemented by the modeling engine and is thus not in the tidymodels team‚Äôs control. In contrast to overhead, the elapsed time of this code is very much sensitive to the size of the inputted data; depending on the engine, increases in the number of rows or columns of training or testing data may drastically increase the time to train or predict from a given model.\n\n\n\n\n\n\nNote\n\n\n\nThe algorithmic complexity of the models implemented by these engines is well-understood in many cases. At the same time, the behavior of elapsed time for some engine implementations often differs greatly from what theory would lead one to believe. Regressions in modeling code may introduce undue slowdowns and, conversely, performance optimizations that lead to elapsed times that scale better than theory would suggest may be the very reason for the existence of some engines.\n\n\nAs shown in Figure¬†1.2, the proportion of elapsed time that overhead is responsible for depends on how quickly the engine can fit or predict for a given dataset.\n\n\n\n\n\n\nFigure¬†1.2\n\n\n\nSince the absolute overhead of tidymodels‚Äô translation is relatively constant, overhead is only a substantial portion of elapsed time when models fit or predict very quickly. For a linear model fitted on 30 data points with lm(), this overhead is continuously benchmarked to remain under 2/3. That is, absolute worst-case, fitting a model with tidymodels takes three times longer than using the engine interface itself. However, this overhead approaches fractions of a percent for fits on even 10,000 rows for many engines. Thus, a focus on reducing the elapsed time of overhead is valuable in the sense that the framework ought not to unintentionally introduce regressions that cause overhead to scale with the size of training data, but in general, the hard part of reducing elapsed time when evaluating models is reducing the elapsed time for computations carried out by the modeling engine.\nThe next question is then how could tidymodels cut down on elapsed time for modeling engines that it doesn‚Äôt own? To answer this question, let‚Äôs revisit the applied example from Section 1.1.2. In that first example, the code does some translation to the engine‚Äôs syntax, sets up some error handling, and then fits and predicts from 120 models.\n\n\n\n\n\n\nFigure¬†1.3\n\n\n\nFigure¬†1.3 depicts this process, where we evaluate all `120 models in order. Each white dot in the engine portion of the elapsed time represents another round of fitting and predicting with engine. Remember that in reality, for even modest dataset sizes, the green portions representing tidymodels overhead are much smaller by proportion than represented.\nIn Section 1.1.3, the first thing I did was introduce a parallel backend. Distributing engine fits across available cores is itself a gamechanger, as illustrated in Figure¬†1.4.\n\n\n\n\n\n\nFigure¬†1.4\n\n\n\nThen, switching out computational engines for a more performant alternative further reduces elapsed time, as shown in ?fig-parallel-resample-opt.\n\nFinally, as depicted in ?fig-parallel-resample-opt2, the submodel trick described in Chapter 5 and racing described in Chapter 4 eliminate a substantial portion of the engine fits.\n\nThe tidymodels team devotes substantial energy to ensuring support for the most performant parallelization technologies, modeling engines, model-specific optimizations, and search techniques. This book will demonstrate how to best make use of these features to reduce the time needed to evaluate machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-datasets",
    "href": "intro.html#sec-datasets",
    "title": "1¬† Introduction",
    "section": "1.4 Datasets",
    "text": "1.4 Datasets\nIn Section 1.1.1, I used a function simulate_classification() to generate data. This is one of two functions, the other being simulate_regression(), that create the data underlying many of the experiments in this book.\nThese two functions are adaptations of their similarly named friends sim_classification() and sim_regression() from the modeldata package. They make small changes to those function‚Äînamely, introducing factor predictors with some tricky distributions‚Äîthat surface slowdowns with some modeling engines.\nProvided a number of rows, sim_classification() generates a tibble with that many rows and 16 columns:\n\nd_class &lt;- simulate_classification(1000)\n\nd_class\n\n# A tibble: 1,000 √ó 18\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;   &lt;fct&gt;               &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_1 level_2          -0.133   level_3            0.0355       0.770 \n 2 class_1 level_2           0.894   level_3            0.356        0.690 \n 3 class_2 level_2          -1.59    level_4            0.249        0.650 \n 4 class_1 level_1           2.17    level_3            0.879        0.0747\n 5 class_1 level_2           0.464   level_1            0.318        0.903 \n 6 class_2 level_2          -2.04    level_3            0.321        0.133 \n 7 class_2 level_2           1.11    level_3            0.848        0.211 \n 8 class_2 level_1          -0.183   level_3            0.381        0.155 \n 9 class_2 level_1           0.00202 level_3            0.275        0.0545\n10 class_2 level_2           0.198   level_3            0.918        0.715 \n# ‚Ñπ 990 more rows\n# ‚Ñπ 12 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;fct&gt;, linear_11 &lt;fct&gt;,\n#   linear_12 &lt;fct&gt;\n\n\nThe leftmost column, class, is a binary outcome variable, and the remaining columns are predictor variables. The predictors throw a few curveballs at the modeling functions we‚Äôll benchmark in this book.\nFor one, the predictors are moderately correlated. Correlated predictors can lead to unstable parameter estimates and can make the model more sensitive to small changes in the data. Further, correlated predictors may lead to slower convergence in gradient descent processes (like those driving gradient-boosted trees like XGBoost and LightGBM), as the resulting elongated and narrow surface of loss functions causes the algorithm to zigzag towards the optimum value, significantly increasing the training time along the way.\n\n\n\n\n\n\n\n\n\nSecondly, there are a number of factor predictors. Some modeling engines experience slower training times with many factor predictors for a variety of reasons. For one, most modeling engines ultimately implement training routines on numeric matrices, requiring that factor predictors are somehow encoded as numbers. Most often in R, this is in the form of treatment contrasts, where an \\(n\\)-length factor with \\(l\\) levels is represented as an \\(n ~x~ l-1\\) matrix composed of zeroes and ones. Each column is referred to as a dummy variable. The first column has value \\(1\\) when the \\(i\\)-th entry of the factor is the second level, zero otherwise. The second column has value \\(1\\) when the \\(i\\)-th entry of the factor is the third level, zero otherwise. We know that the \\(i\\)-th entry of the first took its first level if all of the entries in the \\(i\\)th column of the resulting matrix are zero. While this representation of a factor is relatively straightforward, it‚Äôs quite memory intensive; a factor with 100 levels ultimately will require a 99-column matrix to be allocated in order to be included in a model. While many modeling engines in R assume that factors will be encoded as treatment constrasts, different modeling engines have different approaches to processing factor variables, some more efficient than others. More on this in Section 6.2, in particular.\n\n\nOriginal factor\n\n\n  x\n1 a\n2 b\n3 c\n\n\n\n\n\nFactor with treatment contrasts\n\n\n  xb xc\n1  0  0\n2  1  0\n3  0  1\n\n\n\n\nFurther, many of those factor variables have a class imbalance; that is, some levels of the factor occur much more often than others. Some models may struggle to learn from the less frequently-occurring classes, potentially requiring more iterations of descent processes for some models to converge. Even when this is not the case, it may be varyingly ‚Äúworth it‚Äù in terms of memory usage to allocate a dummy variable to a factor level that only appears a couple times in a dataset with many rows.\n\n\n\n\n\nTODO: write caption\n\n\n\n\nThe regression dataset looks quite similar.\n\nd_reg &lt;- simulate_regression(1000)\n\nd_reg\n\n# A tibble: 1,000 √ó 16\n   outcome predictor_01 predictor_02 predictor_03 predictor_04 predictor_05\n     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1    6.36       -2.45         2.46          2.85        2.16        -1.28 \n 2   -2.11       -4.08         1.64         -3.49        3.21        -3.18 \n 3   26.0        -4.95         1.77         -2.21        3.35         5.29 \n 4   -1.31       -3.36        -2.95         -4.05       -0.911        5.88 \n 5   53.8         0.604       -6.35         -4.89       -5.96        -7.86 \n 6  -32.1         3.53        -6.56          2.06       -0.965       -2.11 \n 7   24.8         2.59         1.68         -1.88        3.10        -1.35 \n 8   11.7         3.79        -1.46         -2.23       -0.467       -0.662\n 9    7.19        4.88         1.08          4.17        3.01        -4.15 \n10    9.43        2.78        -0.441         1.62        0.452        1.74 \n# ‚Ñπ 990 more rows\n# ‚Ñπ 10 more variables: predictor_06 &lt;dbl&gt;, predictor_07 &lt;dbl&gt;,\n#   predictor_08 &lt;dbl&gt;, predictor_09 &lt;dbl&gt;, predictor_10 &lt;fct&gt;,\n#   predictor_11 &lt;fct&gt;, predictor_12 &lt;fct&gt;, predictor_13 &lt;fct&gt;,\n#   predictor_14 &lt;fct&gt;, predictor_15 &lt;fct&gt;\n\n\nThe left-most column, outcome, is a numeric outcome, and the remaining 15 columns are a mix of numeric and factor. The same story related to correlation and tricky factor imbalances goes for the regression dataset. Demonstrating that is homework.\n\n\n\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with R. \" O‚ÄôReilly Media, Inc.\".\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O‚ÄôReilly Media, Inc.\".",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "2¬† Models",
    "section": "",
    "text": "2.1 Tidymodels overhead\nWhile the tidymodels team develops the infrastructure that users interact with directly, under the hood, we send calls out to other people‚Äôs modeling packages‚Äîor modeling engines‚Äîthat provide the actual implementations that estimate parameters, generate predictions, etc. The process looks something like this:\nWhen thinking about the time allotted to each of the three steps above, we refer to the ‚Äútranslate‚Äù steps in green as the tidymodels overhead. The time it takes to ‚Äútranslate‚Äù interfaces in steps 1) and 3) is within our control, while the time the modeling engine takes to do it‚Äôs thing in step 2) is not.\nLet‚Äôs demonstrate with an example classification problem. Generating some random data:\nset.seed(1)\nd &lt;- simulate_classification(n_rows = 100)\n\nd\n\n# A tibble: 100 √ó 18\n   class   two_factor_1 two_factor_2 non_linear_1 non_linear_2 non_linear_3\n   &lt;fct&gt;   &lt;fct&gt;               &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n 1 class_2 level_1          -1.17    level_1             0.554        0.814\n 2 class_1 level_1           0.261   level_2             0.688        0.929\n 3 class_2 level_1          -1.61    level_1             0.658        0.147\n 4 class_1 level_1           2.14    level_1             0.663        0.750\n 5 class_2 level_1           0.0360  level_1             0.472        0.976\n 6 class_1 level_2          -0.00837 level_1             0.970        0.975\n 7 class_2 level_1           1.05    level_2             0.402        0.351\n 8 class_1 level_1           1.49    level_1             0.850        0.394\n 9 class_2 level_1           0.967   level_2             0.757        0.951\n10 class_2 level_2           0.603   level_1             0.533        0.107\n# ‚Ñπ 90 more rows\n# ‚Ñπ 12 more variables: linear_01 &lt;dbl&gt;, linear_02 &lt;dbl&gt;, linear_03 &lt;dbl&gt;,\n#   linear_04 &lt;dbl&gt;, linear_05 &lt;dbl&gt;, linear_06 &lt;dbl&gt;, linear_07 &lt;dbl&gt;,\n#   linear_08 &lt;dbl&gt;, linear_09 &lt;dbl&gt;, linear_10 &lt;fct&gt;, linear_11 &lt;fct&gt;,\n#   linear_12 &lt;fct&gt;\n‚Ä¶we‚Äôd like to model the class using the remainder of the variables in this dataset using a logistic regression. We can using the following code to do so:\nfit(logistic_reg(), class ~ ., d)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = class ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n        (Intercept)  two_factor_1level_2         two_factor_2  \n           7.080808            -7.463598            -2.821078  \nnon_linear_1level_2         non_linear_2         non_linear_3  \n           0.550041            -0.878752            -2.039599  \n          linear_01            linear_02            linear_03  \n          -0.153653            -0.273809            -0.005318  \n          linear_04            linear_05            linear_06  \n           1.248566             0.880917            -0.112520  \n          linear_07            linear_08            linear_09  \n          -0.925063             1.261782            -1.251249  \n   linear_10level_2     linear_10level_3     linear_10level_4  \n         -18.020101            -3.242219            -1.336644  \n   linear_11level_2     linear_11level_3  \n          -0.640718            -4.402850  \n [ reached getOption(\"max.print\") -- omitted 4 entries ]\n\nDegrees of Freedom: 99 Total (i.e. Null);  76 Residual\nNull Deviance:      134.6 \nResidual Deviance: 54.75    AIC: 102.8\nThe default engine for a logistic regression in tidymodels is stats::glm(). So, in the style of the above graphic, this code:\nAgain, we can control what happens in steps 1) and 3), but step 2) belongs to the stats package.\nThe time that steps 1) and 3) take is relatively independent of the dimensionality of the training data. That is, regardless of whether we train on one hundred or a million data points, our code (as in, the translation) takes about the same time to run. Regardless of training set size, our code pushes around small, relational data structures to determine how to correctly interface with a given engine. The time it takes to run step 2), though, depends almost completely on the size of the data. Depending on the modeling engine, modeling 10 times as much data could result in step 2) taking twice as long, or 10x as long, or 100x as long as the original fit.\nSo, while the absolute time allotted to steps 1) and 3) is fixed, the portion of total time to fit a model with tidymodels that is ‚Äúoverhead‚Äù depends on how quick the engine code itself is. How quick is a logistic regression with glm() on 100 data points?\nbench::mark(\n  fit = glm(class ~ ., family = binomial, data = d)\n) %&gt;% \n  select(expression, median)\n\n# A tibble: 1 √ó 2\n  expression   median\n* &lt;bch:expr&gt; &lt;bch:tm&gt;\n1 fit           2.4ms\nAbout a millisecond. That means that, if the tidymodels overhead is one second, we‚Äôve made this model fit a thousand times slower!\nIn practice, the overhead here has hovered around a millisecond or two for the last couple years, and machine learning practitioners usually fit much more computationally expensive models than a logistic regression on 100 data points. You‚Äôll just have to believe me on that second point. Regarding the first:\nbm_logistic_reg &lt;- \n  bench::mark(\n    parsnip = fit(logistic_reg(), class ~ ., d),\n    stats = glm(class ~ ., family = binomial, data = d),\n    check = FALSE\n  )\nRemember that the first expression calls the second one, so the increase in time from the second to the first is the ‚Äúoverhead.‚Äù In this case, it‚Äôs 0.9072275 milliseconds, or 27.6% of the total elapsed time.\nSo, to fit a boosted tree model on 1,000,000 data points, step 2) might take a few seconds. Steps 1) and 3) don‚Äôt care about the size of the data, so they still take a few thousandths of a second. No biggie‚Äîthe overhead is negligible. Let‚Äôs quickly back that up by fitting boosted tree models on simulated datasets of varying sizes, once with the XGBoost interface and once with parsnip‚Äôs wrapper around it.\nTODO: write caption\nThis graph shows the gist of tidymodels‚Äô overhead for modeling engines: as dataset size and model complexity grow larger, model fitting and prediction take up increasingly large proportions of the total evaluation time.\nSection 1.1.3 showed a number of ways users can cut down on the evaluation time of their tidymodels code. Making use of parallelism, reducing the total number of model fits needed to search a given grid, and carefully constructing that grid to search over are all major parts of the story",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#tidymodels-overhead",
    "href": "models.html#tidymodels-overhead",
    "title": "2¬† Models",
    "section": "",
    "text": "A graphic representing the tidymodels interface. In order, step 1 ‚Äútranslate‚Äù, step 2 ‚Äúcall‚Äù, and step 3 ‚Äútranslate‚Äù, outline the process of translating from the standardized tidymodels interface to an engine‚Äôs specific interface, calling the modeling engine, and translating back to the standardized tidymodels interface. Step 1 and step 3 are in green, while step 2 is in orange.\n\n\n\n\n\n\n\n\n\nTranslates the tidymodels code, which is consistent across engines, to the format that is specific to the chosen engine. In this case, there‚Äôs not a whole lot to do: it passes the preprocessor as formula, the data as data, and picks a family of stats::binomial.\nCalls stats::glm() and collects its output.\nTranslates the output of stats::glm() back into a standardized model fit object.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "models.html#benchmarks",
    "href": "models.html#benchmarks",
    "title": "2¬† Models",
    "section": "2.2 Benchmarks",
    "text": "2.2 Benchmarks\n\n2.2.1 Linear models\n\n\n2.2.2 Decision trees\n\n\n2.2.3 Boosted trees\nXGBoost and LightGBM ‚Äì comparison timings for the same thing but from the Python interface?\n\n\n2.2.4 Random forests\n\n\n2.2.5 Support vector machines",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Models</span>"
    ]
  },
  {
    "objectID": "parallelism.html",
    "href": "parallelism.html",
    "title": "3¬† Parallel computing",
    "section": "",
    "text": "3.1 Across-models\nIn Chapter 1, one of the changes I made to greatly speed up that resampling process was to introduce an across-models parallel backend. By ‚Äúacross-models,‚Äù I mean that each individual model fit happens on a single CPU core, but I allot each of the CPU cores I‚Äôve reserved for training a number of model fits to take care of.\nPhrased another way, in the case of ‚Äúsequential‚Äù training, all 120 model fits happened one after the other.\nWhile the one CPU core running my R process works itself to the bone, the other remaining 9 are mostly sitting idle (besides keeping my many browser tabs whirring). CPU parallelism is about somehow making use of more cores than the one my main R process is running on; in theory, if \\(n\\) times as many cores are working on fitting models, the whole process could take \\(1/n\\) of the time.\nWhy do I say ‚Äúin theory‚Äù? The orchestration of splitting up that work is actually a very, very difficult problem, for two main reasons:\nThere are two dominant approaches to distributing model fits across local cores that I‚Äôve hinted at already: forking and socket clusters. We‚Äôll delve further into the weeds of each of those approaches in the coming subsections. At a high level, though, the folk knowledge is that forking is subject to less overhead in sending data back and forth, but has some quirks that make it less portable (more plainly, it‚Äôs not available on Windows) and a bit unstable thanks to a less-than-friendly relationship with R‚Äôs garbage collector. As for load balancing, the choice between these two parallelism techniques isn‚Äôt really relevant.\nBefore I spend time experimenting with these techniques, I want to quickly situate the terminology I‚Äôm using here in the greater context of discussions of parallel computing with R. ‚ÄúSequential,‚Äù ‚Äúforking,‚Äù and ‚Äúsocket clusters‚Äù are my preferred terms for the techniques I‚Äôll now write about, but there‚Äôs quite a bit of diversity in the terminology folks use to refer to them. I‚Äôve also called out keywords (as in, functions or packages) related to these techniques in various generations of parallel computing frameworks in R. In ‚Äúbase,‚Äù I refer to functions in the parallel package, building on popular packages multicore (first on CRAN in 2009, inspiring mclapply()) and snow (first on CRAN in 2003, inspiring parLapply()) and included in base installations of R from 2011 onward (R Core Team 2024).",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "parallelism.html#sec-across-models",
    "href": "parallelism.html#sec-across-models",
    "title": "3¬† Parallel computing",
    "section": "",
    "text": "Figure¬†3.1\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.2\n\n\n\n\n\nGetting data from one place to another: Each of us has likely spent minutes or even hours waiting for a call to load() on a big old .RData object to complete. In that situation, data is being brought in from our hard disk or a remote database (or even elsewhere in memory) into memory allocated by our R process. R is a single-process program, so in order to train multiple models simultaneously, we need multiple R processes, each able to access the training data. We then have two options: one would be to somehow allow each of those R processes to share one copy of the data (this is the idea behind forking, described in Section 3.1.2), the other to send a copy of the data to each process (the idea behind socket clusters, described in Section 3.1.3). The former sounds nice but can become a headache quite quickly. The latter sounds computationally expensive but, with enough memory and sufficiently low latency in copying data (as would be the case with a set of clusters living on one laptop), can often outperform forking for local workflows.\nLoad balancing: Imagine I have some machine learning model with a hyperparameter \\(p\\), and that the computational complexity of that model is such that, with hyperparameter value \\(p\\), the model takes \\(p\\) minutes to train. I am trying out values of \\(p\\) in \\(1, 2, 3, ..., 40\\) and distributing model training across 5 cores. Without the knowledge of how \\(p\\) affects training times, I might send models with \\(p\\) in \\(1, 2, 3, ...8\\) off to the first core, \\(9, 10, 11, ..., 16\\) off to the second core, and so on. In this case, the first core would finish up all of its fits in a little over half an hour while the last would take almost 5 hours. In this example, I‚Äôve taken the penalty on overhead of sending all of the training data off to each core, but in the end, one core ends up doing the majority of the work anyway. In this case, too, we were lucky that the computational complexity of model fits relative to this parameter were roughly linear‚Äîit‚Äôs not uncommon for model fit times to have a quadratic or geometric relationship with the values of important hyperparameters. A critical reader might have two questions. The first: if the computational complexity relative to this parameter is known, why don‚Äôt you just batch the values of \\(p\\) up such that each worker will take approximately the same amount of time? This is a reasonable question, and it relates to the hard problem of chunking. In some situations, related to individual parameters, it really is just about this simple to determine the relationship between parameter values and fit times. In reality, those relationships tend not to be quite so clear-cut, and even when they are, the implications of that parameter value for fit times often depend on the values of other parameters; a pairing of some parameter value \\(p\\) with some other value of a different parameter \\(q\\) might cause instability in some gradient descent process or otherwise, making the problem of estimating the fit time of a model given some set of parameter values a pretty difficult problem for some model types. The second question: couldn‚Äôt you just send each of the cores a single parameter value and have them let the parent R process know they‚Äôre done, at which point they‚Äôll receive another parameter value to get to work on evaluating? That way, the workers that happen to end up with a quicker-fitting values earlier on won‚Äôt sit idle waiting for other cores to finish. This approach is called asynchronous (or ‚Äúasync‚Äù) and, in some situations, can be quite helpful. Remember, though, that this requires getting data (in the form of the communication that a given worker is done evaluating a model, and maybe passing along some performance metric values) back and forth much more often. If the overhead of that communication exceeds the time that synchronous workers had spent idle, waiting for busier cores to finish running, then the asynchronous approach will result in a net slowdown.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nfuture\nforeach\nbase\nSynonyms\n\n\n\n\nSequential\nsequential()\n\n\nSerial\n\n\nForking\nmulticore()\ndoMC\nmclapply()\nProcess forking\n\n\nSocket Clusters\nmultisession()\ndoParallel\nparLapply()\nParallel Socket Clusters (PSOCK), Cluster, Socket\n\n\n\n\n3.1.1 Sequential\nGenerally, in this chapter, I‚Äôm writing about various approaches to parallel computing. I‚Äôll compare each of those approaches to each, but also to the sequential (or ‚Äúnot parallel‚Äù) approach. Sequential evaluation means evaluating model fits in sequence, or one after the other.\nTo demonstrate the impacts of different parallelism approaches throughout this chapter, we‚Äôll always start the conversation with a short experiment. I‚Äôll define a function that tracks the elapsed time to resample a boosted tree ensemble against simulated data, given a number of rows to simulate and a parallelism approach.\n\ntime_resample_bt &lt;- function(n_rows, plan) {\n  # simulate data with n_rows rows\n  set.seed(1)\n  d &lt;- simulate_regression(n_rows)\n  \n  # set up a parallelism plan\n  # set `workers = 4`, which will be ignored for `plan = \"sequential\"`.\n  if (plan == \"multicore\") {\n    rlang::local_options(parallelly.fork.enable = TRUE)\n  }\n  if (plan == \"multisession\") {\n    rlang::local_options(future.globals.maxSize = 1024*1024^2) # 1gb\n  }\n  suppressWarnings(\n    plan(plan, workers = 4) \n  )\n  \n  # track the elapsed time to...\n  bench::mark(\n    resample =\n      fit_resamples(\n        # ...evaluate a boosted tree ensemble...\n        boost_tree(\"regression\"),\n        # ...modeling the outcome using all predictors...\n        outcome ~ .,\n        # ...against a 10-fold cross-validation of `d`.\n        vfold_cv(d, v = 10)\n      ),\n    memory = FALSE\n  )\n}\n\nHere‚Äôs a quick example, simulating 100 rows of data and evaluating its resamples sequentially:\n\nt_seq &lt;- time_resample_bt(100, \"sequential\")\n\nt_seq\n\n\nt_seq\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      201ms    2.19MB\n\n\nIn total, the whole process took 0.2 seconds on my laptop. This expression, among other things, fits a model for each resample on \\(n * \\frac{v-1}{v} = 100 * \\frac{9}{10} = 90\\) rows, meaning that even if the model fits took up 100% of the evaluation time in total, they take 0.02 seconds each. In other words, these fits are quite fast. As such, the overhead of distributing computations across cores would have to be quite minimal in order to see speedups with computations done in parallel. Scaling up the number of rows in the training data, though, results in elapsed times becoming a bit more cumbersome; in the following code, we‚Äôll resample models on datasets with 100 to a million rows, keeping track of the elapsed time for each iteration.\n\npress_seq &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"sequential\"),\n    n_rows = 10^(2:6)\n  )\n\nFor now, the graph we can put together with this data isn‚Äôt super interesting:\n\nggplot(press_seq) +\n  aes(x = n_rows, y = median) +\n  scale_x_log10() +\n  geom_line(color = \"#cd6f3d\") +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption\n\n\n\n\nMore rows means a longer fit time‚Äîwhat a thrill! In the following sections, we‚Äôll compare this timing to those resulting from different parallelism approaches.\n\n\n3.1.2 Forking\nProcess forking is a mechanism where an R process creates an exact copy of itself, called a ‚Äúchild‚Äù process (or ‚Äúworker.‚Äù) Initially, workers share memory with the original (‚Äúparent‚Äù) process, meaning that there‚Äôs no overhead resulting from creating multiple copies of training data to send out to workers. So, in the case of tidymodels, each worker needs to have some modeling packages loaded and some training data available to get started on evaluating a model workflow against resample; those packages and data are already available in the parent process, so tidymodels should see very little overhead in shipping data off to workers with forking.\n\nThere are a few notable drawbacks of process forking:\n\nThere‚Äôs no direct way to ‚Äúfork‚Äù a process from one machine to another, so forking is available only on a single machine. To distribute computations across multiple machines, practitioners will need to make use of socket clusters (described in the following section Section 3.1.3).\nForking is based on the operating system command fork, available only on Unix-alikes (i.e.¬†macOS and Linux). Windows users are out of luck.\nIn practice, memory that is initially shared often ends up ultimately copied due to R‚Äôs garbage collection. ‚Äú[I]f the garbage collector starts running in one of the forked [workers], or the [parent] process, then that originally shared memory can no longer be shared and the operating system starts copying memory blocks into each [worker]. Since the garbage collector runs whenever it wants to, there is no simple way to avoid this‚Äù (Bengtsson 2021).\n\nLet‚Äôs rerun that experiment from the previous section using forking and compare timings.\n\npress_fork &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"multicore\"),\n    n_rows = 10^(2:6)\n  )\n\nWe can make the plot from the last section a bit more interesting now.\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\")\n) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption\n\n\n\n\nWell, if that ain‚Äôt by the book! For the smallest training dataset, n = 100, distributing computations across cores resulted in a new slowdown. Very quickly, though, forking meets up with the sequential approach in elapsed time, and by the time we‚Äôve made it to more realistic dataset sizes, forking almost always wins. In case the log scale is tripping you up, here are the raw timings for the largest dataset:\n\n\n# A tibble: 2 √ó 3\n    median Approach    n_rows\n* &lt;bch:tm&gt; &lt;chr&gt;        &lt;dbl&gt;\n1    4.13m Sequential 1000000\n2    1.34m Forking    1000000\n\n\n\nAnother angle from which to poke at this is how much time did we lose to overhead? Said another way, if I distributed these computations across 4 cores, then I should see a 4-fold speedup in a perfect world. If I divide the time it takes to resample this model sequentially by 4, how does it compare to the timing I observe with forking?\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\")\n) %&gt;%\n  mutate(median_adj = case_when(\n    Approach == \"Sequential\" ~ median / 4,\n    .default = median\n  )) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median_adj, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time Per Core)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption\n\n\n\n\nPer core, no parallel approach will ever be faster than sequential. (If it is, there‚Äôs a performance bug in your code!) As the computations that happen in workers take longer and longer, though, the overhead shrinks as a proportion of the total elapsed time.\n\n\n3.1.3 Socket Clusters\n\nLet‚Äôs see how this plays out in practice. Returning to the same experiment from before:\n\npress_sc &lt;-\n  bench::press(\n    time_resample_bt(n_rows, \"multisession\"),\n    n_rows = 10^(2:6)\n  )\n\n\nbind_rows(\n  mutate(press_seq, Approach = \"Sequential\"),\n  mutate(press_fork, Approach = \"Forking\"),\n  mutate(press_sc, Approach = \"Socket Clusters\")\n) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = median, col = Approach) +\n  scale_x_log10() +\n  geom_line() +\n  labs(y = \"Log(Elapsed Time)\", x = \"Number of Rows\")\n\n\n\n\nTODO: write caption",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "parallelism.html#sec-within-models",
    "href": "parallelism.html#sec-within-models",
    "title": "3¬† Parallel computing",
    "section": "3.2 Within-models",
    "text": "3.2 Within-models\nI‚Äôve only written so far about across-model parallelism, where multiple CPU cores are used to train a set of models but individual model fits happen on a single core. For most machine learning models available via tidymodels, this is the only type of parallelism possible. However, some of the most well-used modeling engines in tidymodels, like XGBoost and LightGBM, allow for distributing the computations to train a single model across several CPU cores (or even on GPUs). This begs the question, then, of whether tidymodels users should always stick to across-model parallelism, to within-model parallelism for the models it‚Äôs available for, or to a hybrid of both. We‚Äôll focus first on the former two options in this subsection and then explore their interactions in Section 3.3.\n\n3.2.1 CPU\n\n\n3.2.2 GPU\nXGBoost parallelizes at a finer level (individual trees and split finding), while LightGBM parallelizes at a coarser level (features and data subsets).\nXGBoost\n\narg device is cpu or cuda (or gpu, but cuda is the only supported device).\narg nthread is integer\nuses openMP for cpu (?)\ngpu_hist is apparently pretty ripping?\n\nLightGBM\n\ndevice_type is cpu, gpu, or cuda (fastest, but requires GPUs supporting CUDA)\ncuda is fastest but only available on Linux with NVIDIA GPUs with compute capability 6.0+\ngpu is based on OpenCL‚Ä¶ M1 Pro is a ‚Äúbuilt-in‚Äù / ‚Äúintegrated‚Äù\ncpu uses OpenMP for CPU parallelism\nset to real CPU cores (i.e.¬†not threads)\nrefer to Installation Guide to build LightGBM with GPU or CUDA support\nare num_threads is integer\nDask available to Python users\n\naorsf\n\nn_thread implements OpenMP CPU threading\n\nkeras (MLP)\n\nCPU or GPU\ncuda (available only for jax backend?)\nalso available with tensorflow\n\nh2o stuff\nbaguette\n\ncontrol_bag(allow_parallel)? does this respect nested parallelism?\n\nWhile GPU support is available for both libraries in R, it‚Äôs not as straightforward to use as in Python. The R ecosystem generally has less robust GPU support compared to Python, which can make GPU-accelerated machine learning in R more challenging to set up and use.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "parallelism.html#sec-within-and-across",
    "href": "parallelism.html#sec-within-and-across",
    "title": "3¬† Parallel computing",
    "section": "3.3 Within and Across",
    "text": "3.3 Within and Across",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "parallelism.html#strategy",
    "href": "parallelism.html#strategy",
    "title": "3¬† Parallel computing",
    "section": "3.4 Strategy",
    "text": "3.4 Strategy\nChoosing n cores and parallel_over‚Ä¶",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "parallelism.html#sec-distributed-computing",
    "href": "parallelism.html#sec-distributed-computing",
    "title": "3¬† Parallel computing",
    "section": "3.5 Distributed Computing",
    "text": "3.5 Distributed Computing\nSo far in this chapter, I‚Äôve mostly focused on the performance considerations for distributing computations across cores on a single computer. Distributed computing ‚Äúin the cloud,‚Äù where data is shipped off for processing on several different computers, is a different ball-game. That conversation is mostly outside of the scope of this book, but I do want to give some high-level intuition on how local parallelism differs from distributed parallelism.\nThe idea that will get you the most mileage in reasoning about distributed parallelism is this: the overhead of sending data back and forth is much more substantial in distributed computing than it is in the local context.\nIn order for me to benefit from distributing these computations across cores, the overhead of sending data out to workers has to be so minimal that it doesn‚Äôt overtake the time saved in distributing model fits across cores. Let‚Äôs see how this plays out on my laptop, first:\n\nt_par &lt;- time_resample_bt(100, \"multisession\")\n\nt_par\n\n\nt_par\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 resample      242ms      15MB\n\n\nNo dice! As we know from earlier on, though, we‚Äôll start to see a payoff when model fits take long enough to outweigh the overhead of sending data back and forth between workers. But, but! Remember the high-mileage lesson: this overhead is greater for distributed systems. Let‚Äôs demonstrate this.\nI‚Äôll first run this experiment for numbers of rows \\(100, 1000, ..., 1,000,000\\) both sequentially and via socket clusters on my laptop, recording the timings as I do so. Then, I‚Äôll do the same thing on a popular data science hosted service, and we‚Äôll compare results.\n\nbench::press(\n  time_resample_bt(n_rows, plan),\n  n_rows = 10^(2:6),\n  plan = c(\"sequential\", \"multisession\")\n)\n\nThe resulting timings are in the object timings and look like this:\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  head()\n\n# A tibble: 6 √ó 4\n   n_rows plan           median system\n*   &lt;dbl&gt; &lt;chr&gt;        &lt;bch:tm&gt; &lt;chr&gt; \n1     100 sequential   216.08ms laptop\n2    1000 sequential   442.77ms laptop\n3   10000 sequential      2.81s laptop\n4  100000 sequential      27.8s laptop\n5 1000000 sequential      4.03m laptop\n6     100 multisession 247.13ms laptop\n\n\nThere‚Äôs one timing per unique combination of number of rows, parallelism plan, and system (\"laptop\" vs \"cloud\"). Based on these timings, we can calculate the factor of speedup for sequential evaluation versus its parallel analogue.\n\ntimings %&gt;%\n  select(n_rows, plan, median, system) %&gt;%\n  pivot_wider(names_from = plan, values_from = median) %&gt;%\n  mutate(\n    speedup = as.numeric(sequential / multisession),\n    speedup = if_else(speedup &lt; 1, -1/speedup, speedup)\n  ) %&gt;%\n  ggplot() +\n  aes(x = n_rows, y = speedup, col = system) +\n  geom_point() +\n  scale_x_log10() +\n  labs(\n    x = \"Log(Number of Rows)\", \n    y = \"Signed Factor of Speedup\", \n    col = \"System\"\n  )\n\n\n\n\nTODO: write caption\n\n\n\n\nIn this plot, a signed speedup value of 2 would mean that the socket cluster (i.e., parallel) approach was twice as fast, while a value of -2 would mean that the sequential approach ran twice as fast as the socket cluster approach. In general, larger numbers of rows (and thus longer-running model fits) tend to be associated with greater speedups as a result of switching to parallel computing. For local clusters on my laptop, the overhead of passing data around is small enough that I start to see a payoff when switching to parallel computing for only 1000 rows. As for the cloud system, though, model fits have to take a long time before switching to parallel computing begins to reduce elapsed times.\nThe conclusion to draw here is not that one ought not to use hosted setups for parallel computing. Aside from the other many benefits of doing data analysis in hosted environments, some of these environments enable ‚Äúmassively parallel‚Äù computing, or, in other words, a ton of cores. In the example shown in this chapter, we fitted a relatively small number of models, so we wouldn‚Äôt necessarily benefit (and would likely see a slowdown) from scaling up the number of cores utilized. If we had instead wanted to tune that boosted tree over 1,000 proposed hyperparameter combinations‚Äîrequiring 10,000 model fits with a 10-fold resampling scheme‚Äîwe would likely be much better off utilizing a distributed system with higher latency (and maybe even less performant individual cores) and 1,000 or 10,000 cores.\n\n\n\n\nBengtsson, Henrik. 2021. ‚ÄúA Unifying Framework for Parallel and Distributed Processing in R using Futures.‚Äù The R Journal 13 (2): 273‚Äì91. https://doi.org/10.32614/RJ-2021-048.\n\n\nR Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Parallel computing</span>"
    ]
  },
  {
    "objectID": "search.html",
    "href": "search.html",
    "title": "4¬† Search",
    "section": "",
    "text": "4.1 Grid search",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "search.html#iterative-search",
    "href": "search.html#iterative-search",
    "title": "4¬† Search",
    "section": "4.2 Iterative Search",
    "text": "4.2 Iterative Search\n\n4.2.1 Simulated Annealing\n\n\n4.2.2 Bayesian Optimization",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Search</span>"
    ]
  },
  {
    "objectID": "submodel.html",
    "href": "submodel.html",
    "title": "5¬† The submodel trick",
    "section": "",
    "text": "5.1 Demonstration\nRecall that, in Section 1.1.2, we resampled an XGBoost boosted tree model to predict whether patients would be readmitted within 30 days after an inpatient hospital stay. Using default settings with no optimizations, the model took 3.68 hours to resample. With a switch in computation engine, implementation of parallel processing, change in search strategy, and enablement of the submodel trick, the time to resample was knocked down to 1.52 minutes. What is the submodel trick, though, and what was its individual contribution to that speedup?\nFirst, loading packages and setting up resamples and the model specification as before:\n# load packages\nlibrary(tidymodels)\nlibrary(readmission)\n\n# load and split data:\nset.seed(1)\nreadmission_split &lt;- initial_split(readmission)\nreadmission_train &lt;- training(readmission_split)\nreadmission_test &lt;- testing(readmission_split)\nreadmission_folds &lt;- vfold_cv(readmission_train)\n\n# set up model specification\nbt &lt;- \n  boost_tree(learn_rate = tune(), trees = tune()) %&gt;%\n  set_mode(\"classification\")",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "submodel.html#demonstration",
    "href": "submodel.html#demonstration",
    "title": "5¬† The submodel trick",
    "section": "",
    "text": "5.1.1 Grids\nIf I just pass these resamples and model specification to tune_grid(), as I did in Section 1.1.2, tidymodels will take care of generating the grid of parameters to evaluate itself. By default, tidymodels generates grids of parameters using an experimental design called a latin hypercube (McKay, Beckman, and Conover 2000; Dupuy, Helbert, and Franco 2015). We can use the function grid_latin_hypercube() from the dials package to replicate the same grid that tune_grid() had generated under the hood:\n\nset.seed(1)\nbt_grid_latin_hypercube &lt;- \n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;%\n  grid_latin_hypercube(size = 12)\n\nbt_grid_latin_hypercube\n\n# A tibble: 12 √ó 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1   647    0.164  \n 2   404    0.0555 \n 3  1798    0.0452 \n 4   796    0.274  \n 5  1884    0.00535\n 6  1139    0.00137\n 7  1462    0.105  \n 8  1201    0.00194\n 9   867    0.0225 \n10   166    0.00408\n11   326    0.0116 \n12  1608    0.00997\n\n\nSince we‚Äôre working with a two-dimensional grid in this case, we can plot the resulting grid to get a sense for the distribution of values:\n\nggplot(bt_grid_latin_hypercube) +\n  aes(x = trees, y = learn_rate) +\n  geom_point()\n\n\n\n\nTODO: write caption\n\n\n\n\nWhile the details of sampling using latin hypercubes are not important to understand this chapter, note that values are not repeated in a given dimension. Said another way, we get 12 unique values for trees, and 12 unique values for learn_rate. Juxtapose this with the design resulting from grid_regular() used in Section 1.1.3:\n\nset.seed(1)\nbt_grid_regular &lt;-\n  bt %&gt;%\n  extract_parameter_set_dials() %&gt;% \n  grid_regular(levels = 4)\n\nbt_grid_regular\n\n# A tibble: 16 √ó 2\n   trees learn_rate\n   &lt;int&gt;      &lt;dbl&gt;\n 1     1    0.001  \n 2   667    0.001  \n 3  1333    0.001  \n 4  2000    0.001  \n 5     1    0.00681\n 6   667    0.00681\n 7  1333    0.00681\n 8  2000    0.00681\n 9     1    0.0464 \n10   667    0.0464 \n11  1333    0.0464 \n12  2000    0.0464 \n13     1    0.316  \n14   667    0.316  \n15  1333    0.316  \n16  2000    0.316  \n\n\nPlotting in the same way:\n\nggplot(bt_grid_regular) +\n  aes(x = trees, y = learn_rate) +\n  geom_point()\n\n\n\n\nTODO: write caption\n\n\n\n\nThe argument levels = 4 indicates that \\(4\\) values are generated individually for each parameter, and the resulting grid is created by pairing up each unique combination of those values.\nYou may have noticed that this regular grid contains even more proposed points‚Äî4 x 4 = 16‚Äîthan the latin hypercube with size = 12. A reasonable question, then: how on earth would the larger grid be resampled more quickly than the smaller one? It‚Äôs possible that I hid some slowdown resulting from this larger grid among the rest of the optimizations implemented in Section 1.1.3; lets test the effect of the change in grid by itself.\n\nset.seed(1)\n\nbm_grid_regular &lt;- \n  bench::mark(\n    grid_regular = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid_regular\n      )\n  )\n\n\nbm_grid_regular\n\n# A tibble: 1 √ó 3\n  expression   median mem_alloc\n* &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 basic         1.23h    3.91GB\n\n\nSee? Have some faith in me!\nChanging only the grid argument (and even increasing the number of proposed grid points), we‚Äôve decreased the time to evaluate against resamples from 3.68 to 1.23 hours, or a speedup of -99.67%.\n\n\n5.1.2 The trick\nPassing this regular grid allowed tune_grid() to use what the tidymodels team refers to as the ‚Äúsubmodel trick,‚Äù where many more models can be evaluated than were actually fit.\nTo best understand how the submodel trick works, let‚Äôs refresh on how boosted trees work. The training process begins with a simple decision tree, and subsequent trees are added iteratively, each one correcting the errors of the previous trees by focusing more on the data points associated with the greatest error. The final model is a weighted sum of all the individual trees, where each tree contributes to reducing the overall error.\nSo, for example, to train a boosted tree model with 2000 trees, we first need to train a boosted tree model with 1 tree. Then, we need to take that model, figure out where it made its largest errors, and train a second tree that aims to correct those errors. So on, until the, say, 667-th tree, and so on until the 1333-rd tree, and so on until, finally, the 2000-th tree. Picking up what I‚Äôm putting down? Along the way to training a boosted tree with 2000 trees, we happened to train a bunch of other models we might be interested in evaluating: what we call submodels. So, in the example of bt_grid_regular, for a given learn_rate, we only need to train the model with the maximum trees. In this example, that‚Äôs a quarter of the model fits.\n\n\n\n\n\n\nNote\n\n\n\nYou might note that we don‚Äôt see a speedup nearly as drastic as 4 times. While we indeed only fit a quarter of the models, we‚Äôre fitting the boosted trees with the largest number of trees, and the time to train a boosted tree scales linearly with the number of trees. Said another way, we‚Äôre eliminating the need to fit only the faster-fitting models. This tends to be the case in many cases where the submodel trick applies.\n\n\nTo evaluate a fitted model with performance metrics, all we need are its predictions (and, usually, the true values being predicted). In pseudocode, resampling a model against performance metrics usually goes something like this:\n\nfor (resample in resamples) {\n  # analogue to the \"training\" set for the resample\n  analysis &lt;- analysis(resample)\n  # analogue to the \"testing\" set for the resample\n  assessment &lt;- assessment(resample)\n  \n  for (model in models) {\n    # the longest-running operation:\n    model_fit &lt;- fit(model, analysis)\n    \n    # usually, comparatively quick operations:\n    model_predictions &lt;- predict(model_fit, assessment)\n    metrics &lt;- c(metrics, metric(model_predictions))\n  }\n}\n\n\n\n\n\n\n\nNote\n\n\n\nanalysis(), assessment(), fit(), and predict() are indeed actual functions in tidymodels. metric() is not, but an analogue could be created from the output of the function metric_set().\n\n\nAmong all of these operations, fitting the model with fit() is usually the longest-running step, by far. In comparison, predict()ing on new values and calculating metrics takes very little time. Using the submodel trick allows us to reduce the number of fit()s while keeping the number of calls to predict() and metric() constant. In pseudocode, resampling with the submodel trick could look like:\n\nfor (resample in resamples) {\n  analysis &lt;- analysis(resample)\n  assessment &lt;- assessment(resample)\n  \n  models_to_fit &lt;- models[unique(non_submodel_args)]\n  \n  for (model in models_to_fit) {\n    model_fit &lt;- fit(model, analysis)\n    \n    for (model_to_eval in models[unique(submodel_args)]) {\n      model_to_eval &lt;- predict(model_to_eval, assessment)\n      metrics &lt;- c(metrics, metric(model_predictions))\n    }\n  }\n}\n\nThe above pseudocode admittedly requires some generosity (or mental gymnastics) to interpret, but the idea is that if fit()ting is indeed the majority of the time spent in resampling, and models_to_fit contains many fewer elements than models in the preceding pseudocode blocks, we should see substantial speedups.\n\n\n5.1.3 At its most extreme\nIn the applied example above, we saw a relatively modest speedup. If we want to really show off the power of the submodel trick, in terms of time spent resampling per model evaluated, we can come up with a somewhat silly grid:\n\nbt_grid_regular_go_brrr &lt;-\n  bt_grid_regular %&gt;%\n  slice_max(trees, by = learn_rate) %&gt;%\n  map(.x = c(1, seq(10, max(.$trees), 10)), .f = ~mutate(.y, trees = .x), dfr = .) %&gt;%\n  bind_rows()\n\nbt_grid_regular_go_brrr\n\n# A tibble: 804 √ó 2\n   trees learn_rate\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1     1    0.001  \n 2     1    0.00681\n 3     1    0.0464 \n 4     1    0.316  \n 5    10    0.001  \n 6    10    0.00681\n 7    10    0.0464 \n 8    10    0.316  \n 9    20    0.001  \n10    20    0.00681\n# ‚Ñπ 794 more rows\n\n\nIn this grid, we have the same number of unique values of learn_rate, \\(4\\), resulting in the same \\(4\\) model fits. Except that, in this case, we‚Äôre evaluating every model with number of trees \\(1, 10, 20, 30, 40, ..., 2000\\). If our hypothesis that predicting on the assessment set and generating performance metrics is comparatively fast is true, then we‚Äôll see that elapsed time per grid point is way lower:\n\nset.seed(1)\n\nbm_grid_regular_go_brrr &lt;- \n  bench::mark(\n    grid_regular_go_brrr = \n      tune_grid(\n        object = bt,\n        preprocessor = readmitted ~ .,\n        resamples = readmission_folds,\n        grid = bt_grid_regular_go_brrr\n      )\n  )\n\nThe above code took 1.44 hours to run, a bit longer than the 1.23 hours from bm_grid_regular (that fitted the same number of models), but comparable. Per grid point, that difference is huge:\n\n# time per grid point for bm_grid_regular\nbm_grid_regular$median[[1]] / nrow(bt_grid_regular)\n\n[1] 4.62m\n\n# time per grid point for bm_grid_regular_go_brrr\nbm_grid_regular_go_brrr$median[[1]] / nrow(bt_grid_regular_go_brrr)\n\n[1] 6.44s\n\n\nWhether this difference in timing is of practical significance to a user is debatable. In the context where we generated bm_grid_regular, where the grid of points searched over is relatively comparable (and thus similarly likely to identify a performant model) yet the decreased number of model fits gave rise to a reasonable speedup‚Äî-99.67%‚Äîis undoubtedly impactful for many typical use cases of tidymodels. The more eye-popping per-grid-point speedups, as with bm_grid_regular_go_brrr, are more so a fun trick than a practical tool for most use cases.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "submodel.html#overview",
    "href": "submodel.html#overview",
    "title": "5¬† The submodel trick",
    "section": "5.2 Overview",
    "text": "5.2 Overview\nI‚Äôve demonstrated the impact of the submodel trick in the above section through one example context, tuning trees in an XGBoost boosted tree model. The submodel trick can be found in many places across the tidymodels framework, though.\nSubmodels are defined with respect to a given model parameter, e.g.¬†trees in boost_tree(). While a given parameter often defines a submodel regardless of modeling engine for a given model type‚Äîe.g.¬†trees defines submodels for boost_tree() models regardless of whether the model is fitted with engine = \"xgboost\" or engine = \"lightgbm\"‚Äîthere are some exceptions. Many tidymodels users undoubtedly tune models using arguments that define submodels without even knowing it. Some common examples include:\n\npenalty in linear_reg(), logistic_reg(), and multinom_reg(), which controls the amount of regularization in linear models.\nneighbors in nearest_neighbor(), the number of training data points nearest the point to be predicted that are factored into the prediction.\n\nAgain, some modeling engines for each of these model types do not actually support prediction from submodels from the noted parameter. See Section 5.2.1 for a complete table of currently supported arguments defining submodels in tidymodels.\nIn the above example, we had to manually specify a grid like bt_regular_grid in order for tidymodels to use a grid that can take advantage of the submodel trick. This reflects the frameworks‚Äô general prioritization of predictive performance over computational performance in its design and defaults; latin hypercubes have better statistical properties when it comes to discovering performant hyperparameter combinations than a regular grid (McKay, Beckman, and Conover 2000; Stein 1987; Santner et al. 2003). However, note that in the case where a model is being tuned over only one argument, the submodel trick will kick in regardless of the sampling approach being used: regardless of how a set of univariate points are distributed, the most extreme parameter value (e.g.¬†the max trees) can be used to generate predictions for values across the distribution.\n\n5.2.1 Supported parameters\nA number of tuning parameters support the submodel trick:\n\n\n\n\n\nModel Type\nArgument\nEngines\n\n\n\n\nboost_tree\ntrees\nxgboost, C5.0, lightgbm\n\n\nC5_rules\ntrees\nC5.0\n\n\ncubist_rules\nneighbors\nCubist\n\n\ndiscrim_flexible\nnum_terms\nearth\n\n\nlinear_reg\npenalty\nglmnet\n\n\nlogistic_reg\npenalty\nglmnet\n\n\nmars\nnum_terms\nearth\n\n\nmultinom_reg\npenalty\nglmnet\n\n\nnearest_neighbor\nneighbors\nkknn\n\n\npls\nnum_comp\nmixOmics\n\n\npoisson_reg\npenalty\nglmnet\n\n\nproportional_hazards\npenalty\nglmnet\n\n\nrule_fit\npenalty\nxrf\n\n\n\n\n\n\n\n\n\nDupuy, Delphine, C√©line Helbert, and Jessica Franco. 2015. ‚ÄúDiceDesign and DiceEval: Two r Packages for Design and Analysis of Computer Experiments.‚Äù Journal of Statistical Software 65 (11).\n\n\nMcKay, Michael D, Richard J Beckman, and William J Conover. 2000. ‚ÄúA Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.‚Äù Technometrics 42 (1): 55‚Äì61.\n\n\nSantner, Thomas J, Brian J Williams, William I Notz, and Brain J Williams. 2003. The Design and Analysis of Computer Experiments. Vol. 1. Springer.\n\n\nStein, Michael. 1987. ‚ÄúLarge Sample Properties of Simulations Using Latin Hypercube Sampling.‚Äù Technometrics 29 (2): 143‚Äì51.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>The submodel trick</span>"
    ]
  },
  {
    "objectID": "extras.html",
    "href": "extras.html",
    "title": "6¬† Extras",
    "section": "",
    "text": "6.1 Preprocessing",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "extras.html#sec-sparsity",
    "href": "extras.html#sec-sparsity",
    "title": "6¬† Extras",
    "section": "6.2 Sparsity",
    "text": "6.2 Sparsity",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "extras.html#sec-gpu-training",
    "href": "extras.html#sec-gpu-training",
    "title": "6¬† Extras",
    "section": "6.3 GPU Training",
    "text": "6.3 GPU Training",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  },
  {
    "objectID": "extras.html#sec-stacking",
    "href": "extras.html#sec-stacking",
    "title": "6¬† Extras",
    "section": "6.4 Stacking",
    "text": "6.4 Stacking",
    "crumbs": [
      "Advanced",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Extras</span>"
    ]
  }
]